{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712e27c6",
   "metadata": {},
   "source": [
    "# Sistema de Recomendaci√≥n ALS - Entrenamiento en Kaggle\n",
    "## MovieLens-20M Dataset\n",
    "\n",
    "Este notebook implementa un sistema completo de entrenamiento, reentrenamiento incremental y predicci√≥n para el modelo ALS usando PySpark.\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Entrenamiento escalable en Kaggle con PySpark\n",
    "- Reentrenamiento incremental autom√°tico\n",
    "- Sistema de versionado de modelos\n",
    "- Descarga y predicci√≥n local\n",
    "- Monitoreo y validaci√≥n autom√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c1724",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n del Entorno y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de dependencias\n",
    "!pip install -q pyspark==3.5.0\n",
    "!pip install -q kaggle\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "!pip install -q joblib\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "print(\"‚úì Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff963c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas y variables globales\n",
    "class Config:\n",
    "    \"\"\"Configuraci√≥n centralizada del sistema\"\"\"\n",
    "    \n",
    "    # Rutas base\n",
    "    BASE_PATH = Path(\"/kaggle/working\")\n",
    "    INPUT_PATH = Path(\"/kaggle/input/movielens-20m-dataset\")\n",
    "    MODEL_PATH = BASE_PATH / \"models\"\n",
    "    METRICS_PATH = BASE_PATH / \"metrics\"\n",
    "    LOGS_PATH = BASE_PATH / \"logs\"\n",
    "    \n",
    "    # Configuraci√≥n de Spark\n",
    "    SPARK_MEMORY = \"14g\"  # Kaggle tiene ~16GB RAM\n",
    "    SPARK_CORES = 4\n",
    "    SPARK_PARTITIONS = 200\n",
    "    \n",
    "    # Par√°metros del modelo ALS\n",
    "    DEFAULT_RANK = 10\n",
    "    DEFAULT_MAX_ITER = 5\n",
    "    DEFAULT_REG_PARAM = 0.1\n",
    "    DEFAULT_ALPHA = 1.0\n",
    "    \n",
    "    # Configuraci√≥n de reentrenamiento\n",
    "    INCREMENTAL_THRESHOLD = 0.1  # 10% de nuevos datos\n",
    "    MODEL_VERSION_FORMAT = \"als_model_v{version}_{date}\"\n",
    "    MAX_MODEL_VERSIONS = 5\n",
    "    \n",
    "    # M√©tricas\n",
    "    EVALUATION_METRICS = [\"rmse\", \"mae\"]\n",
    "    TOP_K = [5, 10, 20]\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_directories(cls):\n",
    "        \"\"\"Crear directorios necesarios\"\"\"\n",
    "        for path in [cls.MODEL_PATH, cls.METRICS_PATH, cls.LOGS_PATH]:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úì Directorios creados en {cls.BASE_PATH}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_system_info(cls):\n",
    "        \"\"\"Obtener informaci√≥n del sistema\"\"\"\n",
    "        return {\n",
    "            \"spark_memory\": cls.SPARK_MEMORY,\n",
    "            \"spark_cores\": cls.SPARK_CORES,\n",
    "            \"spark_partitions\": cls.SPARK_PARTITIONS,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "Config.setup_directories()\n",
    "print(f\"‚úì Configuraci√≥n inicializada\")\n",
    "print(f\"  - Memoria Spark: {Config.SPARK_MEMORY}\")\n",
    "print(f\"  - Cores: {Config.SPARK_CORES}\")\n",
    "print(f\"  - Particiones: {Config.SPARK_PARTITIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar SparkSession con configuraci√≥n optimizada\n",
    "def create_spark_session(app_name=\"ALS-MovieLens-Training\"):\n",
    "    \"\"\"\n",
    "    Crea una sesi√≥n de Spark optimizada para Kaggle\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.memory\", Config.SPARK_MEMORY) \\\n",
    "        .config(\"spark.executor.memory\", Config.SPARK_MEMORY) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", Config.SPARK_PARTITIONS) \\\n",
    "        .config(\"spark.default.parallelism\", Config.SPARK_CORES * 2) \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    print(\"‚úì Spark Session creada exitosamente\")\n",
    "    print(f\"  - Spark Version: {spark.version}\")\n",
    "    print(f\"  - Master: {spark.sparkContext.master}\")\n",
    "    print(f\"  - App Name: {spark.sparkContext.appName}\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb993c1",
   "metadata": {},
   "source": [
    "## 2. Carga y Preprocesamiento de Datos MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813059fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Maneja la carga y preprocesamiento de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, spark, input_path=Config.INPUT_PATH):\n",
    "        self.spark = spark\n",
    "        self.input_path = input_path\n",
    "        self.ratings_df = None\n",
    "        self.movies_df = None\n",
    "        self.stats = {}\n",
    "    \n",
    "    def load_ratings(self):\n",
    "        \"\"\"Carga el dataset de ratings\"\"\"\n",
    "        print(\"Cargando ratings...\")\n",
    "        \n",
    "        ratings_file = self.input_path / \"rating.csv\"\n",
    "        \n",
    "        if not ratings_file.exists():\n",
    "            raise FileNotFoundError(f\"No se encontr√≥ el archivo: {ratings_file}\")\n",
    "        \n",
    "        # Leer CSV con inferencia de esquema\n",
    "        self.ratings_df = self.spark.read.csv(\n",
    "            str(ratings_file),\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        # Convertir tipos expl√≠citamente para asegurar compatibilidad con ALS\n",
    "        self.ratings_df = self.ratings_df.select(\n",
    "            F.col(\"userId\").cast(IntegerType()).alias(\"userId\"),\n",
    "            F.col(\"movieId\").cast(IntegerType()).alias(\"movieId\"),\n",
    "            F.col(\"rating\").cast(FloatType()).alias(\"rating\"),\n",
    "            F.col(\"timestamp\").cast(LongType()).alias(\"timestamp\")\n",
    "        )\n",
    "        \n",
    "        # Verificar que no est√© vac√≠o\n",
    "        initial_count = self.ratings_df.count()\n",
    "        if initial_count == 0:\n",
    "            raise ValueError(\"El dataset de ratings est√° vac√≠o\")\n",
    "        \n",
    "        print(f\"‚úì Ratings cargados: {initial_count:,} registros\")\n",
    "        \n",
    "        # Reparticionar para mejor distribuci√≥n\n",
    "        self.ratings_df = self.ratings_df.repartition(\n",
    "            Config.SPARK_PARTITIONS, \n",
    "            \"userId\"\n",
    "        )\n",
    "        \n",
    "        return self.ratings_df\n",
    "    \n",
    "    def load_movies(self):\n",
    "        \"\"\"Carga informaci√≥n de pel√≠culas\"\"\"\n",
    "        print(\"Cargando informaci√≥n de pel√≠culas...\")\n",
    "        \n",
    "        movies_file = self.input_path / \"movie.csv\"\n",
    "        \n",
    "        if movies_file.exists():\n",
    "            self.movies_df = self.spark.read.csv(\n",
    "                str(movies_file),\n",
    "                header=True,\n",
    "                inferSchema=True\n",
    "            )\n",
    "            \n",
    "            count = self.movies_df.count()\n",
    "            print(f\"‚úì Pel√≠culas cargadas: {count:,} registros\")\n",
    "        else:\n",
    "            print(\"‚ö† Archivo de pel√≠culas no encontrado\")\n",
    "        \n",
    "        return self.movies_df\n",
    "    \n",
    "    def explore_data(self):\n",
    "        \"\"\"Explora y genera estad√≠sticas del dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPLORACI√ìN DE DATOS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if self.ratings_df is None:\n",
    "            print(\"‚ö† No hay datos cargados\")\n",
    "            return\n",
    "        \n",
    "        # Estad√≠sticas b√°sicas\n",
    "        self.stats['total_ratings'] = self.ratings_df.count()\n",
    "        self.stats['unique_users'] = self.ratings_df.select(\"userId\").distinct().count()\n",
    "        self.stats['unique_movies'] = self.ratings_df.select(\"movieId\").distinct().count()\n",
    "        \n",
    "        print(f\"\\nTotal de Ratings: {self.stats['total_ratings']:,}\")\n",
    "        print(f\"Usuarios √∫nicos: {self.stats['unique_users']:,}\")\n",
    "        print(f\"Pel√≠culas √∫nicas: {self.stats['unique_movies']:,}\")\n",
    "        \n",
    "        # Sparsity\n",
    "        sparsity = 1.0 - (\n",
    "            self.stats['total_ratings'] / \n",
    "            (self.stats['unique_users'] * self.stats['unique_movies'])\n",
    "        )\n",
    "        self.stats['sparsity'] = sparsity\n",
    "        print(f\"Sparsity: {sparsity:.4%}\")\n",
    "        \n",
    "        # Distribuci√≥n de ratings\n",
    "        print(\"\\nDistribuci√≥n de Ratings:\")\n",
    "        rating_dist = self.ratings_df.groupBy(\"rating\").count() \\\n",
    "            .orderBy(\"rating\") \\\n",
    "            .collect()\n",
    "        \n",
    "        for row in rating_dist:\n",
    "            print(f\"  Rating {row['rating']}: {row['count']:,}\")\n",
    "        \n",
    "        # Estad√≠sticas de ratings\n",
    "        stats = self.ratings_df.select(\n",
    "            F.mean(\"rating\").alias(\"mean\"),\n",
    "            F.stddev(\"rating\").alias(\"std\"),\n",
    "            F.min(\"rating\").alias(\"min\"),\n",
    "            F.max(\"rating\").alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\nEstad√≠sticas de Ratings:\")\n",
    "        print(f\"  Media: {stats['mean']:.2f}\")\n",
    "        print(f\"  Desv. Est√°ndar: {stats['std']:.2f}\")\n",
    "        print(f\"  M√≠nimo: {stats['min']:.1f}\")\n",
    "        print(f\"  M√°ximo: {stats['max']:.1f}\")\n",
    "        \n",
    "        # Usuarios y pel√≠culas m√°s activos\n",
    "        print(\"\\nTop 5 Usuarios m√°s activos:\")\n",
    "        top_users = self.ratings_df.groupBy(\"userId\").count() \\\n",
    "            .orderBy(F.desc(\"count\")) \\\n",
    "            .limit(5) \\\n",
    "            .collect()\n",
    "        \n",
    "        for i, row in enumerate(top_users, 1):\n",
    "            print(f\"  {i}. Usuario {row['userId']}: {row['count']} ratings\")\n",
    "        \n",
    "        print(\"\\nTop 5 Pel√≠culas m√°s valoradas:\")\n",
    "        top_movies = self.ratings_df.groupBy(\"movieId\").count() \\\n",
    "            .orderBy(F.desc(\"count\")) \\\n",
    "            .limit(5) \\\n",
    "            .collect()\n",
    "        \n",
    "        for i, row in enumerate(top_movies, 1):\n",
    "            print(f\"  {i}. Pel√≠cula {row['movieId']}: {row['count']} ratings\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self.stats\n",
    "    \n",
    "    def clean_data(self):\n",
    "        \"\"\"Limpia y valida los datos\"\"\"\n",
    "        print(\"Limpiando datos...\")\n",
    "        \n",
    "        initial_count = self.ratings_df.count()\n",
    "        \n",
    "        # Eliminar nulos\n",
    "        self.ratings_df = self.ratings_df.na.drop()\n",
    "        \n",
    "        # Validar rango de ratings\n",
    "        self.ratings_df = self.ratings_df.filter(\n",
    "            (F.col(\"rating\") >= 0.5) & (F.col(\"rating\") <= 5.0)\n",
    "        )\n",
    "        \n",
    "        # Eliminar duplicados\n",
    "        self.ratings_df = self.ratings_df.dropDuplicates([\"userId\", \"movieId\"])\n",
    "        \n",
    "        # Validar que userId y movieId no sean nulos o negativos\n",
    "        self.ratings_df = self.ratings_df.filter(\n",
    "            (F.col(\"userId\").isNotNull()) & \n",
    "            (F.col(\"movieId\").isNotNull()) &\n",
    "            (F.col(\"userId\") > 0) &\n",
    "            (F.col(\"movieId\") > 0)\n",
    "        )\n",
    "        \n",
    "        final_count = self.ratings_df.count()\n",
    "        removed = initial_count - final_count\n",
    "        \n",
    "        if final_count == 0:\n",
    "            raise ValueError(\"Todos los datos fueron removidos durante la limpieza. Verifica el dataset.\")\n",
    "        \n",
    "        print(f\"‚úì Limpieza completada\")\n",
    "        print(f\"  - Registros removidos: {removed:,}\")\n",
    "        print(f\"  - Registros finales: {final_count:,}\")\n",
    "        \n",
    "        return self.ratings_df\n",
    "    \n",
    "    def prepare_train_test_split(self, train_ratio=0.8, seed=42):\n",
    "        \"\"\"Divide datos en entrenamiento y prueba\"\"\"\n",
    "        print(f\"\\nDividiendo datos (train: {train_ratio:.0%}, test: {1-train_ratio:.0%})...\")\n",
    "        \n",
    "        # Verificar que hay datos antes de dividir\n",
    "        total_count = self.ratings_df.count()\n",
    "        if total_count == 0:\n",
    "            raise ValueError(\"No hay datos para dividir. El DataFrame est√° vac√≠o.\")\n",
    "        \n",
    "        train_df, test_df = self.ratings_df.randomSplit(\n",
    "            [train_ratio, 1-train_ratio], \n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Persistir en memoria para evitar recomputaci√≥n\n",
    "        train_df = train_df.persist()\n",
    "        test_df = test_df.persist()\n",
    "        \n",
    "        train_count = train_df.count()\n",
    "        test_count = test_df.count()\n",
    "        \n",
    "        if train_count == 0:\n",
    "            raise ValueError(\"El conjunto de entrenamiento est√° vac√≠o despu√©s de la divisi√≥n\")\n",
    "        \n",
    "        if test_count == 0:\n",
    "            raise ValueError(\"El conjunto de prueba est√° vac√≠o despu√©s de la divisi√≥n\")\n",
    "        \n",
    "        print(f\"‚úì Divisi√≥n completada\")\n",
    "        print(f\"  - Train: {train_count:,} registros ({train_count/total_count:.1%})\")\n",
    "        print(f\"  - Test: {test_count:,} registros ({test_count/total_count:.1%})\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "\n",
    "# Inicializar y cargar datos\n",
    "data_loader = DataLoader(spark)\n",
    "ratings_df = data_loader.load_ratings()\n",
    "movies_df = data_loader.load_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar y limpiar datos\n",
    "stats = data_loader.explore_data()\n",
    "ratings_df = data_loader.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e991e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar splits de datos\n",
    "train_df, test_df = data_loader.prepare_train_test_split(train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar datos antes del entrenamiento\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICACI√ìN PRE-ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataFrame de entrenamiento:\")\n",
    "print(f\"  - Registros: {train_df.count():,}\")\n",
    "print(f\"  - Columnas: {train_df.columns}\")\n",
    "print(f\"  - Schema:\")\n",
    "train_df.printSchema()\n",
    "\n",
    "print(f\"\\nMuestra de datos de entrenamiento:\")\n",
    "train_df.show(10)\n",
    "\n",
    "print(f\"\\nEstad√≠sticas b√°sicas:\")\n",
    "train_df.describe().show()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95291584",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento Inicial del Modelo ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee364a7f",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento Inicial del Modelo ALS\n",
    "\n",
    "### üìö Gu√≠a de Entrenamiento: Normal vs Optimizado vs Mejor Modelo\n",
    "\n",
    "#### **üéØ Entrenamiento Normal (R√°pido - 5-8 minutos)**\n",
    "```python\n",
    "model = trainer.train_model(\n",
    "    train_df,\n",
    "    rank=10,          # Factores latentes (baja complejidad)\n",
    "    maxIter=5,        # Pocas iteraciones (convergencia r√°pida)\n",
    "    regParam=0.1      # Regularizaci√≥n moderada\n",
    ")\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- ‚ö° M√°s r√°pido (~5-8 minutos con 20M ratings)\n",
    "- üéØ Bueno para desarrollo y pruebas\n",
    "- üìä RMSE esperado: ~0.85-0.90\n",
    "- üíæ Menos memoria (~2-3 GB)\n",
    "- ‚úÖ **√ösalo para**: Validar pipeline, desarrollo inicial, entrenamiento diario\n",
    "\n",
    "---\n",
    "\n",
    "#### **üî¨ Entrenamiento con Optimizaci√≥n (Grid Search - 30-60 minutos)**\n",
    "```python\n",
    "param_grid = {\n",
    "    'rank': [10, 20, 30],\n",
    "    'regParam': [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "model = trainer.train_with_optimization(\n",
    "    train_df,\n",
    "    validation_df=test_df,\n",
    "    param_grid=param_grid\n",
    ")\n",
    "```\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- üîç Busca la mejor combinaci√≥n de hiperpar√°metros\n",
    "- üìà Prueba m√∫ltiples configuraciones (3√ó3 = 9 entrenamientos)\n",
    "- üéØ Selecciona el modelo con mejor RMSE en validaci√≥n\n",
    "- ‚è±Ô∏è M√°s lento (30-60 min dependiendo del grid)\n",
    "- üìä RMSE esperado: ~0.82-0.87\n",
    "- ‚úÖ **√ösalo para**: Primera vez, ajuste fino, cuando cambias el dataset\n",
    "\n",
    "---\n",
    "\n",
    "#### **üèÜ Mejor Modelo Posible (Producci√≥n - 20-40 minutos)**\n",
    "```python\n",
    "model = trainer.train_model(\n",
    "    train_df,\n",
    "    rank=50,          # M√°s factores latentes = mayor expresividad\n",
    "    maxIter=15,       # M√°s iteraciones = mejor convergencia\n",
    "    regParam=0.05,    # Regularizaci√≥n ajustada (no sobreajustar)\n",
    "    alpha=1.0\n",
    ")\n",
    "```\n",
    "\n",
    "**Configuraci√≥n √ìptima para MovieLens-20M:**\n",
    "- **rank=50**: Captura m√°s patrones complejos de preferencias\n",
    "- **maxIter=15**: Balance convergencia/tiempo\n",
    "- **regParam=0.05**: Evita overfitting sin perder expresividad\n",
    "- **checkpointInterval=5**: Previene StackOverflow en iteraciones largas\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- üéØ M√°xima precisi√≥n posible\n",
    "- üìä RMSE esperado: ~0.78-0.83\n",
    "- üíæ Mayor consumo de memoria (~3.5-4 GB)\n",
    "- ‚è±Ô∏è Tiempo moderado (20-40 min)\n",
    "- ‚úÖ **√ösalo para**: Modelo final de producci√≥n, evaluaci√≥n benchmark\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Comparativa de Par√°metros\n",
    "\n",
    "| Par√°metro | Entrenamiento R√°pido | Optimizaci√≥n | Mejor Modelo | Efecto |\n",
    "|-----------|---------------------|--------------|--------------|--------|\n",
    "| **rank** | 10 | Grid: [10,20,30] | 50 | ‚Üë Mayor expresividad, ‚Üë memoria |\n",
    "| **maxIter** | 5 | 5 (por modelo) | 15 | ‚Üë Mejor convergencia, ‚Üë tiempo |\n",
    "| **regParam** | 0.1 | Grid: [0.01,0.1,1.0] | 0.05 | ‚Üì Menos overfitting |\n",
    "| **RMSE** | ~0.88 | ~0.84 | ~0.80 | ‚Üì Mejor |\n",
    "| **Tiempo** | 5-8 min | 30-60 min | 20-40 min | - |\n",
    "| **Memoria** | ~2.5 GB | ~2.5 GB/modelo | ~4 GB | - |\n",
    "\n",
    "---\n",
    "\n",
    "### üéõÔ∏è Explicaci√≥n de Hiperpar√°metros\n",
    "\n",
    "#### **rank** (Factores Latentes)\n",
    "- **Qu√© es**: Dimensiones del espacio latente (factores ocultos)\n",
    "- **Valores t√≠picos**: 10-100\n",
    "- **Efecto**:\n",
    "  - ‚Üë Mayor rank = M√°s expresividad, captura patrones complejos\n",
    "  - ‚Üì Mayor rank = M√°s memoria, m√°s riesgo de overfitting\n",
    "- **Recomendaci√≥n**: \n",
    "  - rank=10 para desarrollo\n",
    "  - rank=20-30 para producci√≥n est√°ndar\n",
    "  - rank=50 para m√°xima calidad\n",
    "\n",
    "#### **maxIter** (Iteraciones)\n",
    "- **Qu√© es**: N√∫mero de pasadas del algoritmo ALS\n",
    "- **Valores t√≠picos**: 5-20\n",
    "- **Efecto**:\n",
    "  - ‚Üë M√°s iteraciones = Mejor convergencia\n",
    "  - ‚Üì M√°s iteraciones = M√°s tiempo\n",
    "  - Despu√©s de ~15 iter, mejora marginal\n",
    "- **Recomendaci√≥n**:\n",
    "  - maxIter=5 para desarrollo\n",
    "  - maxIter=10 para producci√≥n\n",
    "  - maxIter=15 para m√°xima calidad\n",
    "\n",
    "#### **regParam** (Regularizaci√≥n L2)\n",
    "- **Qu√© es**: Penalizaci√≥n para evitar overfitting\n",
    "- **Valores t√≠picos**: 0.01-1.0\n",
    "- **Efecto**:\n",
    "  - ‚Üë Mayor valor = M√°s generalizaci√≥n, menos overfitting\n",
    "  - ‚Üì Menor valor = M√°s ajuste a datos de entrenamiento\n",
    "- **Recomendaci√≥n**:\n",
    "  - regParam=0.1 para empezar\n",
    "  - regParam=0.01-0.05 si tienes muchos datos\n",
    "  - regParam=0.5-1.0 si ves overfitting\n",
    "\n",
    "#### **alpha** (Confianza Impl√≠cita)\n",
    "- **Qu√© es**: Peso para ratings impl√≠citos\n",
    "- **Valores t√≠picos**: 1.0 (ratings expl√≠citos), 40 (impl√≠citos)\n",
    "- **Efecto**: En MovieLens (expl√≠cito) usar alpha=1.0\n",
    "- **Recomendaci√≥n**: Dejar en 1.0 para ratings expl√≠citos\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Estrategia Recomendada\n",
    "\n",
    "#### **Fase 1: Desarrollo (Primera vez)**\n",
    "```python\n",
    "# 1. Entrenamiento r√°pido para validar\n",
    "model_dev = trainer.train_model(train_df, rank=10, maxIter=5)\n",
    "# Tiempo: ~5 min | RMSE: ~0.88\n",
    "```\n",
    "\n",
    "#### **Fase 2: Optimizaci√≥n (Una vez)**\n",
    "```python\n",
    "# 2. Buscar mejores hiperpar√°metros\n",
    "param_grid = {\n",
    "    'rank': [10, 20, 30],\n",
    "    'regParam': [0.05, 0.1, 0.2]\n",
    "}\n",
    "model_opt = trainer.train_with_optimization(train_df, test_df, param_grid)\n",
    "# Tiempo: ~45 min | RMSE: ~0.84 | Mejores params guardados\n",
    "```\n",
    "\n",
    "#### **Fase 3: Producci√≥n (Modelo Final)**\n",
    "```python\n",
    "# 3. Entrenar con mejores par√°metros encontrados + m√°s recursos\n",
    "model_prod = trainer.train_model(\n",
    "    train_df,\n",
    "    rank=30,      # Del grid search\n",
    "    maxIter=15,   # M√°s iteraciones\n",
    "    regParam=0.05 # Del grid search\n",
    ")\n",
    "# Tiempo: ~25 min | RMSE: ~0.80\n",
    "```\n",
    "\n",
    "#### **Fase 4: Mantenimiento (Diario/Semanal)**\n",
    "```python\n",
    "# 4. Reentrenamiento incremental con par√°metros √≥ptimos\n",
    "model_updated = incremental_trainer.auto_retrain_pipeline(new_ratings_df)\n",
    "# Usa autom√°ticamente los mejores par√°metros guardados\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Consejos Pr√°cticos\n",
    "\n",
    "1. **Primera vez**: Usa entrenamiento r√°pido (rank=10, iter=5) para validar\n",
    "2. **Tienes tiempo**: Corre grid search una vez para encontrar mejores par√°metros\n",
    "3. **Producci√≥n**: Usa rank=30-50, iter=15, regParam del grid search\n",
    "4. **Actualizaciones**: El sistema autom√°tico usa par√°metros guardados\n",
    "5. **Memoria limitada**: Mant√©n rank ‚â§ 20\n",
    "6. **Tiempo limitado**: Usa maxIter=5-8\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Expectativas de Rendimiento\n",
    "\n",
    "| Dataset | Rank | Iter | RMSE | Precision@10 | Tiempo |\n",
    "|---------|------|------|------|--------------|--------|\n",
    "| MovieLens-20M | 10 | 5 | 0.88 | 0.13 | 5-8 min |\n",
    "| MovieLens-20M | 20 | 10 | 0.84 | 0.16 | 12-18 min |\n",
    "| MovieLens-20M | 50 | 15 | 0.80 | 0.19 | 25-35 min |\n",
    "\n",
    "**RMSE < 0.85** = Excelente para sistemas de recomendaci√≥n  \n",
    "**Precision@10 > 0.15** = Buen sistema de recomendaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8104091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ELIGE TU ESTRATEGIA DE ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "# Descomenta UNA de las siguientes opciones:\n",
    "\n",
    "# OPCI√ìN 1: ENTRENAMIENTO R√ÅPIDO (Recomendado para primera vez)\n",
    "# ‚ö° Tiempo: ~5-8 min | üéØ RMSE esperado: ~0.88\n",
    "print(\"üöÄ Estrategia: ENTRENAMIENTO R√ÅPIDO\")\n",
    "print(\"=\" * 60)\n",
    "model = trainer.train_model(\n",
    "    train_df,\n",
    "    rank=10,\n",
    "    maxIter=5,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "# OPCI√ìN 2: ENTRENAMIENTO CON OPTIMIZACI√ìN (Ejecutar una vez para encontrar mejores params)\n",
    "# üî¨ Tiempo: ~45-60 min | üéØ RMSE esperado: ~0.84\n",
    "# print(\"üî¨ Estrategia: OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "# print(\"=\" * 60)\n",
    "# param_grid = {\n",
    "#     'rank': [10, 20, 30],\n",
    "#     'regParam': [0.05, 0.1, 0.2]\n",
    "# }\n",
    "# model = trainer.train_with_optimization(\n",
    "#     train_df,\n",
    "#     validation_df=test_df,\n",
    "#     param_grid=param_grid\n",
    "# )\n",
    "\n",
    "# OPCI√ìN 3: MEJOR MODELO POSIBLE (Para producci√≥n final)\n",
    "# üèÜ Tiempo: ~25-35 min | üéØ RMSE esperado: ~0.80\n",
    "# print(\"üèÜ Estrategia: MEJOR MODELO POSIBLE\")\n",
    "# print(\"=\" * 60)\n",
    "# model = trainer.train_model(\n",
    "#     train_df,\n",
    "#     rank=50,\n",
    "#     maxIter=15,\n",
    "#     regParam=0.05\n",
    "# )\n",
    "\n",
    "print(f\"\\n‚úì Modelo entrenado exitosamente\")\n",
    "print(f\"  - Rank: {model.rank}\")\n",
    "print(f\"  - User factors: {model.userFactors.count():,}\")\n",
    "print(f\"  - Item factors: {model.itemFactors.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee825e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZACI√ìN AVANZADA (Opcional - Solo ejecutar si tienes tiempo)\n",
    "# Tiempo estimado: 2-3 horas\n",
    "# Prueba m√°s combinaciones para encontrar el √≥ptimo absoluto\n",
    "\n",
    "# Descomenta para ejecutar:\n",
    "# print(\"üî¨ OPTIMIZACI√ìN AVANZADA\")\n",
    "# print(\"=\" * 60)\n",
    "# print(\"‚ö†Ô∏è  Esto tomar√° 2-3 horas. Solo ejecuta si tienes tiempo.\")\n",
    "# print()\n",
    "# \n",
    "# param_grid_advanced = {\n",
    "#     'rank': [10, 20, 30, 50],\n",
    "#     'regParam': [0.01, 0.05, 0.1, 0.2],\n",
    "#     'maxIter': 15,\n",
    "#     'alpha': [1.0, 5.0, 10.0],\n",
    "# }\n",
    "# \n",
    "# model_advanced = trainer.train_with_optimization(\n",
    "#     train_df,\n",
    "#     validation_df=test_df,\n",
    "#     param_grid=param_grid_advanced\n",
    "# )\n",
    "# \n",
    "# print(f\"\\n‚úì Mejores par√°metros encontrados:\")\n",
    "# print(f\"  {trainer.best_params}\")\n",
    "# print(f\"  RMSE: {trainer.training_metrics.get('best_rmse', 'N/A'):.4f}\")\n",
    "\n",
    "print(\"üí° Tip: Para optimizaci√≥n avanzada, descomenta el c√≥digo de arriba\")\n",
    "print(\"   Recomendado solo si buscas el m√°ximo rendimiento absoluto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4b7a7",
   "metadata": {},
   "source": [
    "### üìñ Ejemplo de Optimizaci√≥n Avanzada (Opcional)\n",
    "\n",
    "Si quieres explorar m√°s combinaciones de par√°metros, aqu√≠ hay una configuraci√≥n avanzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060cf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSTrainer:\n",
    "    \"\"\"Maneja el entrenamiento del modelo ALS\"\"\"\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.model = None\n",
    "        self.training_metrics = {}\n",
    "        self.best_params = {}\n",
    "    \n",
    "    def build_als_model(self, rank=None, maxIter=None, regParam=None, alpha=None):\n",
    "        \"\"\"\n",
    "        Construye el modelo ALS con par√°metros especificados\n",
    "        \"\"\"\n",
    "        rank = rank or Config.DEFAULT_RANK\n",
    "        maxIter = maxIter or Config.DEFAULT_MAX_ITER\n",
    "        regParam = regParam or Config.DEFAULT_REG_PARAM\n",
    "        alpha = alpha or Config.DEFAULT_ALPHA\n",
    "        \n",
    "        als = ALS(\n",
    "            rank=rank,\n",
    "            maxIter=maxIter,\n",
    "            regParam=regParam,\n",
    "            userCol=\"userId\",\n",
    "            itemCol=\"movieId\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True,\n",
    "            implicitPrefs=False,\n",
    "            alpha=alpha,\n",
    "            seed=42,\n",
    "            checkpointInterval=10\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Modelo ALS configurado:\")\n",
    "        print(f\"  - Rank: {rank}\")\n",
    "        print(f\"  - Max Iterations: {maxIter}\")\n",
    "        print(f\"  - Reg Parameter: {regParam}\")\n",
    "        print(f\"  - Alpha: {alpha}\")\n",
    "        \n",
    "        return als\n",
    "    \n",
    "    def train_model(self, train_df, rank=None, maxIter=None, regParam=None, alpha=None):\n",
    "        \"\"\"\n",
    "        Entrena el modelo ALS\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENTRENAMIENTO DEL MODELO ALS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Validar datos de entrada\n",
    "        print(\"\\nValidando datos de entrenamiento...\")\n",
    "        train_count = train_df.count()\n",
    "        \n",
    "        if train_count == 0:\n",
    "            raise ValueError(\"El DataFrame de entrenamiento est√° vac√≠o\")\n",
    "        \n",
    "        print(f\"‚úì Datos de entrenamiento v√°lidos: {train_count:,} registros\")\n",
    "        \n",
    "        # Verificar esquema\n",
    "        required_cols = [\"userId\", \"movieId\", \"rating\"]\n",
    "        actual_cols = train_df.columns\n",
    "        \n",
    "        for col in required_cols:\n",
    "            if col not in actual_cols:\n",
    "                raise ValueError(f\"Columna requerida '{col}' no encontrada en el DataFrame\")\n",
    "        \n",
    "        print(f\"‚úì Esquema validado: {actual_cols}\")\n",
    "        \n",
    "        # Mostrar muestra de datos\n",
    "        print(\"\\nMuestra de datos:\")\n",
    "        train_df.show(5)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        print(f\"\\nInicio: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Construir modelo\n",
    "        als = self.build_als_model(rank, maxIter, regParam, alpha)\n",
    "        \n",
    "        # Entrenar\n",
    "        print(\"\\nEntrenando modelo...\")\n",
    "        try:\n",
    "            self.model = als.fit(train_df)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚úó Error durante el entrenamiento: {str(e)}\")\n",
    "            print(\"\\nDiagn√≥stico:\")\n",
    "            print(f\"  - Registros: {train_count}\")\n",
    "            print(f\"  - Columnas: {train_df.columns}\")\n",
    "            print(f\"  - Tipos: {train_df.dtypes}\")\n",
    "            raise\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        training_duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Guardar m√©tricas de entrenamiento\n",
    "        self.training_metrics = {\n",
    "            'start_time': start_time.isoformat(),\n",
    "            'end_time': end_time.isoformat(),\n",
    "            'duration_seconds': training_duration,\n",
    "            'rank': als.getRank(),\n",
    "            'max_iter': als.getMaxIter(),\n",
    "            'reg_param': als.getRegParam(),\n",
    "            'alpha': als.getAlpha(),\n",
    "            'train_size': train_count\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úì Entrenamiento completado en {training_duration:.2f} segundos ({training_duration/60:.2f} minutos)\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train_with_optimization(self, train_df, validation_df=None, param_grid=None):\n",
    "        \"\"\"\n",
    "        Entrena con optimizaci√≥n de hiperpar√°metros (opcional)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENTRENAMIENTO CON OPTIMIZACI√ìN\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if param_grid is None:\n",
    "            # Grid simple para b√∫squeda r√°pida\n",
    "            param_grid = {\n",
    "                'rank': [10, 20],\n",
    "                'regParam': [0.1, 0.01]\n",
    "            }\n",
    "        \n",
    "        als = self.build_als_model()\n",
    "        \n",
    "        best_rmse = float('inf')\n",
    "        best_params = {}\n",
    "        best_model = None\n",
    "        \n",
    "        total_combinations = len(param_grid['rank']) * len(param_grid['regParam'])\n",
    "        current = 0\n",
    "        \n",
    "        print(f\"\\nProbando {total_combinations} combinaciones de par√°metros...\")\n",
    "        \n",
    "        for rank in param_grid['rank']:\n",
    "            for regParam in param_grid['regParam']:\n",
    "                current += 1\n",
    "                print(f\"\\n[{current}/{total_combinations}] Entrenando con rank={rank}, regParam={regParam}\")\n",
    "                \n",
    "                # Entrenar modelo\n",
    "                als_temp = self.build_als_model(rank=rank, regParam=regParam, maxIter=5)\n",
    "                model_temp = als_temp.fit(train_df)\n",
    "                \n",
    "                # Evaluar\n",
    "                if validation_df is not None:\n",
    "                    predictions = model_temp.transform(validation_df)\n",
    "                    evaluator = RegressionEvaluator(\n",
    "                        metricName=\"rmse\",\n",
    "                        labelCol=\"rating\",\n",
    "                        predictionCol=\"prediction\"\n",
    "                    )\n",
    "                    rmse = evaluator.evaluate(predictions)\n",
    "                    \n",
    "                    print(f\"  RMSE: {rmse:.4f}\")\n",
    "                    \n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_params = {'rank': rank, 'regParam': regParam}\n",
    "                        best_model = model_temp\n",
    "                        print(f\"  ‚úì Nuevo mejor modelo!\")\n",
    "        \n",
    "        print(f\"\\n‚úì Optimizaci√≥n completada\")\n",
    "        print(f\"  - Mejores par√°metros: {best_params}\")\n",
    "        print(f\"  - Mejor RMSE: {best_rmse:.4f}\")\n",
    "        \n",
    "        self.model = best_model\n",
    "        self.best_params = best_params\n",
    "        self.training_metrics['best_params'] = best_params\n",
    "        self.training_metrics['best_rmse'] = best_rmse\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "# Inicializar trainer\n",
    "trainer = ALSTrainer(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo con par√°metros por defecto\n",
    "model = trainer.train_model(\n",
    "    train_df,\n",
    "    rank=Config.DEFAULT_RANK,\n",
    "    maxIter=Config.DEFAULT_MAX_ITER,\n",
    "    regParam=Config.DEFAULT_REG_PARAM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdad43",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n y M√©tricas del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Eval√∫a el rendimiento del modelo\"\"\"\n",
    "    \n",
    "    def __init__(self, model, spark):\n",
    "        self.model = model\n",
    "        self.spark = spark\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def evaluate_regression_metrics(self, test_df):\n",
    "        \"\"\"\n",
    "        Calcula m√©tricas de regresi√≥n (RMSE, MAE, MSE)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUACI√ìN DE M√âTRICAS DE REGRESI√ìN\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Generar predicciones\n",
    "        predictions = self.model.transform(test_df)\n",
    "        predictions = predictions.na.drop()  # Eliminar cold start\n",
    "        \n",
    "        # RMSE\n",
    "        evaluator_rmse = RegressionEvaluator(\n",
    "            metricName=\"rmse\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        \n",
    "        # MAE\n",
    "        evaluator_mae = RegressionEvaluator(\n",
    "            metricName=\"mae\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        mae = evaluator_mae.evaluate(predictions)\n",
    "        \n",
    "        # MSE\n",
    "        evaluator_mse = RegressionEvaluator(\n",
    "            metricName=\"mse\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        mse = evaluator_mse.evaluate(predictions)\n",
    "        \n",
    "        # R2\n",
    "        evaluator_r2 = RegressionEvaluator(\n",
    "            metricName=\"r2\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        r2 = evaluator_r2.evaluate(predictions)\n",
    "        \n",
    "        self.metrics['rmse'] = rmse\n",
    "        self.metrics['mae'] = mae\n",
    "        self.metrics['mse'] = mse\n",
    "        self.metrics['r2'] = r2\n",
    "        self.metrics['test_size'] = test_df.count()\n",
    "        self.metrics['predictions_count'] = predictions.count()\n",
    "        \n",
    "        print(f\"\\nM√©tricas de Regresi√≥n:\")\n",
    "        print(f\"  - RMSE: {rmse:.4f}\")\n",
    "        print(f\"  - MAE: {mae:.4f}\")\n",
    "        print(f\"  - MSE: {mse:.4f}\")\n",
    "        print(f\"  - R¬≤: {r2:.4f}\")\n",
    "        print(f\"\\nRegistros de test: {test_df.count():,}\")\n",
    "        print(f\"Predicciones v√°lidas: {predictions.count():,}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def evaluate_ranking_metrics(self, test_df, k_values=None):\n",
    "        \"\"\"\n",
    "        Calcula m√©tricas de ranking (Precision@K, Recall@K)\n",
    "        \"\"\"\n",
    "        if k_values is None:\n",
    "            k_values = Config.TOP_K\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUACI√ìN DE M√âTRICAS DE RANKING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Generar recomendaciones para todos los usuarios\n",
    "        print(\"\\nGenerando recomendaciones...\")\n",
    "        user_recs = self.model.recommendForAllUsers(max(k_values))\n",
    "        \n",
    "        # Preparar datos de test (pel√≠culas reales que gustaron)\n",
    "        # Consideramos rating >= 4.0 como positivo\n",
    "        actual = test_df.filter(F.col(\"rating\") >= 4.0) \\\n",
    "            .groupBy(\"userId\") \\\n",
    "            .agg(F.collect_list(\"movieId\").alias(\"actual_movies\"))\n",
    "        \n",
    "        # Extraer IDs de pel√≠culas recomendadas\n",
    "        user_recs = user_recs.withColumn(\n",
    "            \"recommended_movies\",\n",
    "            F.col(\"recommendations.movieId\")\n",
    "        ).select(\"userId\", \"recommended_movies\")\n",
    "        \n",
    "        # Join con datos reales\n",
    "        eval_data = user_recs.join(actual, \"userId\", \"inner\")\n",
    "        \n",
    "        ranking_metrics = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            print(f\"\\nCalculando m√©tricas para K={k}...\")\n",
    "            \n",
    "            # Truncar recomendaciones a K\n",
    "            eval_k = eval_data.withColumn(\n",
    "                \"recommended_k\",\n",
    "                F.slice(\"recommended_movies\", 1, k)\n",
    "            )\n",
    "            \n",
    "            # Calcular Precision@K y Recall@K\n",
    "            eval_k = eval_k.withColumn(\n",
    "                \"hits\",\n",
    "                F.size(F.array_intersect(\"recommended_k\", \"actual_movies\"))\n",
    "            )\n",
    "            \n",
    "            eval_k = eval_k.withColumn(\n",
    "                \"precision\",\n",
    "                F.col(\"hits\") / k\n",
    "            )\n",
    "            \n",
    "            eval_k = eval_k.withColumn(\n",
    "                \"recall\",\n",
    "                F.col(\"hits\") / F.size(\"actual_movies\")\n",
    "            )\n",
    "            \n",
    "            # Promedios\n",
    "            avg_metrics = eval_k.agg(\n",
    "                F.avg(\"precision\").alias(\"avg_precision\"),\n",
    "                F.avg(\"recall\").alias(\"avg_recall\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            precision_k = avg_metrics['avg_precision']\n",
    "            recall_k = avg_metrics['avg_recall']\n",
    "            f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k) if (precision_k + recall_k) > 0 else 0\n",
    "            \n",
    "            ranking_metrics[f'precision@{k}'] = precision_k\n",
    "            ranking_metrics[f'recall@{k}'] = recall_k\n",
    "            ranking_metrics[f'f1@{k}'] = f1_k\n",
    "            \n",
    "            print(f\"  - Precision@{k}: {precision_k:.4f}\")\n",
    "            print(f\"  - Recall@{k}: {recall_k:.4f}\")\n",
    "            print(f\"  - F1@{k}: {f1_k:.4f}\")\n",
    "        \n",
    "        self.metrics.update(ranking_metrics)\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return ranking_metrics\n",
    "    \n",
    "    def evaluate_coverage(self, train_df, test_df, k=10):\n",
    "        \"\"\"\n",
    "        Calcula la cobertura del modelo (% de items recomendados)\n",
    "        \"\"\"\n",
    "        print(\"\\nCalculando cobertura del modelo...\")\n",
    "        \n",
    "        # Total de pel√≠culas √∫nicas\n",
    "        total_movies = train_df.select(\"movieId\").distinct().count()\n",
    "        \n",
    "        # Generar recomendaciones\n",
    "        user_recs = self.model.recommendForAllUsers(k)\n",
    "        \n",
    "        # Pel√≠culas √∫nicas recomendadas\n",
    "        recommended_movies = user_recs.select(\n",
    "            F.explode(\"recommendations.movieId\").alias(\"movieId\")\n",
    "        ).distinct().count()\n",
    "        \n",
    "        coverage = recommended_movies / total_movies\n",
    "        \n",
    "        self.metrics['coverage'] = coverage\n",
    "        self.metrics['total_movies'] = total_movies\n",
    "        self.metrics['recommended_movies'] = recommended_movies\n",
    "        \n",
    "        print(f\"‚úì Cobertura: {coverage:.2%}\")\n",
    "        print(f\"  - Pel√≠culas √∫nicas: {total_movies:,}\")\n",
    "        print(f\"  - Pel√≠culas recomendadas: {recommended_movies:,}\")\n",
    "        \n",
    "        return coverage\n",
    "    \n",
    "    def generate_evaluation_report(self):\n",
    "        \"\"\"\n",
    "        Genera un reporte completo de evaluaci√≥n\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"REPORTE DE EVALUACI√ìN COMPLETO\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\n{'M√©trica':<25} {'Valor':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for metric, value in sorted(self.metrics.items()):\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric:<25} {value:<15.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric:<25} {value:<15,}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def save_metrics(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Guarda las m√©tricas en un archivo JSON\n",
    "        \"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = Config.METRICS_PATH / f\"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        metrics_with_metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'metrics': self.metrics\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(metrics_with_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úì M√©tricas guardadas en: {filepath}\")\n",
    "        \n",
    "        return filepath\n",
    "\n",
    "# Inicializar evaluador\n",
    "evaluator = ModelEvaluator(model, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar m√©tricas de regresi√≥n\n",
    "regression_metrics = evaluator.evaluate_regression_metrics(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar m√©tricas de ranking\n",
    "ranking_metrics = evaluator.evaluate_ranking_metrics(test_df, k_values=[5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar cobertura\n",
    "coverage = evaluator.evaluate_coverage(train_df, test_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb602623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo\n",
    "all_metrics = evaluator.generate_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a7a99",
   "metadata": {},
   "source": [
    "## 5. Guardado y Exportaci√≥n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Gestiona el guardado, carga y versionado de modelos\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=Config.MODEL_PATH):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def get_next_version(self):\n",
    "        \"\"\"\n",
    "        Obtiene el siguiente n√∫mero de versi√≥n\n",
    "        \"\"\"\n",
    "        existing_versions = []\n",
    "        \n",
    "        for folder in self.base_path.iterdir():\n",
    "            if folder.is_dir() and folder.name.startswith(\"als_model_v\"):\n",
    "                try:\n",
    "                    version = int(folder.name.split(\"_v\")[1].split(\"_\")[0])\n",
    "                    existing_versions.append(version)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return max(existing_versions, default=0) + 1\n",
    "    \n",
    "    def save_model(self, model, metrics=None, training_params=None, version=None):\n",
    "        \"\"\"\n",
    "        Guarda el modelo con metadatos\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GUARDANDO MODELO\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Determinar versi√≥n\n",
    "        if version is None:\n",
    "            version = self.get_next_version()\n",
    "        \n",
    "        # Crear nombre del modelo\n",
    "        date_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        model_name = f\"als_model_v{version}_{date_str}\"\n",
    "        model_path = self.base_path / model_name\n",
    "        \n",
    "        print(f\"\\nGuardando modelo: {model_name}\")\n",
    "        print(f\"Ruta: {model_path}\")\n",
    "        \n",
    "        # Guardar modelo de Spark\n",
    "        model_spark_path = model_path / \"spark_model\"\n",
    "        model.write().overwrite().save(str(model_spark_path))\n",
    "        print(f\"‚úì Modelo Spark guardado\")\n",
    "        \n",
    "        # Preparar metadatos\n",
    "        self.metadata = {\n",
    "            'version': version,\n",
    "            'model_name': model_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'spark_version': spark.version,\n",
    "            'model_type': 'ALS',\n",
    "            'model_path': str(model_path),\n",
    "            'metrics': metrics or {},\n",
    "            'training_params': training_params or {},\n",
    "            'system_info': Config.get_system_info()\n",
    "        }\n",
    "        \n",
    "        # Guardar metadatos\n",
    "        metadata_path = model_path / \"metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        print(f\"‚úì Metadatos guardados\")\n",
    "        \n",
    "        # Guardar informaci√≥n del modelo\n",
    "        model_info = {\n",
    "            'rank': model.rank,\n",
    "            'user_factors': model.userFactors.count(),\n",
    "            'item_factors': model.itemFactors.count()\n",
    "        }\n",
    "        \n",
    "        info_path = model_path / \"model_info.json\"\n",
    "        with open(info_path, 'w') as f:\n",
    "            json.dump(model_info, f, indent=2)\n",
    "        print(f\"‚úì Informaci√≥n del modelo guardada\")\n",
    "        \n",
    "        # Calcular checksum\n",
    "        checksum = self._calculate_directory_checksum(model_path)\n",
    "        checksum_path = model_path / \"checksum.txt\"\n",
    "        with open(checksum_path, 'w') as f:\n",
    "            f.write(checksum)\n",
    "        print(f\"‚úì Checksum calculado: {checksum[:16]}...\")\n",
    "        \n",
    "        print(f\"\\n‚úì Modelo guardado exitosamente en: {model_path}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return model_path, self.metadata\n",
    "    \n",
    "    def _calculate_directory_checksum(self, directory):\n",
    "        \"\"\"\n",
    "        Calcula checksum de un directorio\n",
    "        \"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        \n",
    "        for filepath in sorted(Path(directory).rglob('*')):\n",
    "            if filepath.is_file():\n",
    "                hash_md5.update(str(filepath).encode())\n",
    "        \n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Carga un modelo guardado\n",
    "        \"\"\"\n",
    "        print(f\"\\nCargando modelo desde: {model_path}\")\n",
    "        \n",
    "        model_path = Path(model_path)\n",
    "        spark_model_path = model_path / \"spark_model\"\n",
    "        \n",
    "        if not spark_model_path.exists():\n",
    "            raise FileNotFoundError(f\"No se encontr√≥ el modelo en: {spark_model_path}\")\n",
    "        \n",
    "        # Cargar modelo\n",
    "        model = ALSModel.load(str(spark_model_path))\n",
    "        print(f\"‚úì Modelo cargado\")\n",
    "        \n",
    "        # Cargar metadatos\n",
    "        metadata_path = model_path / \"metadata.json\"\n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "            print(f\"‚úì Metadatos cargados (versi√≥n {self.metadata.get('version', 'N/A')})\")\n",
    "        \n",
    "        return model, self.metadata\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"\n",
    "        Lista todos los modelos guardados\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        \n",
    "        for folder in sorted(self.base_path.iterdir()):\n",
    "            if folder.is_dir() and folder.name.startswith(\"als_model_v\"):\n",
    "                metadata_path = folder / \"metadata.json\"\n",
    "                \n",
    "                if metadata_path.exists():\n",
    "                    with open(metadata_path, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    models.append({\n",
    "                        'name': folder.name,\n",
    "                        'path': str(folder),\n",
    "                        'version': metadata.get('version', 'N/A'),\n",
    "                        'timestamp': metadata.get('timestamp', 'N/A'),\n",
    "                        'metrics': metadata.get('metrics', {})\n",
    "                    })\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def cleanup_old_versions(self, keep_versions=None):\n",
    "        \"\"\"\n",
    "        Elimina versiones antiguas del modelo\n",
    "        \"\"\"\n",
    "        if keep_versions is None:\n",
    "            keep_versions = Config.MAX_MODEL_VERSIONS\n",
    "        \n",
    "        models = self.list_models()\n",
    "        \n",
    "        if len(models) <= keep_versions:\n",
    "            print(f\"‚úì Solo hay {len(models)} versiones, no se requiere limpieza\")\n",
    "            return\n",
    "        \n",
    "        # Ordenar por timestamp (m√°s recientes primero)\n",
    "        models.sort(key=lambda x: x['timestamp'], reverse=True)\n",
    "        \n",
    "        # Eliminar versiones antiguas\n",
    "        to_delete = models[keep_versions:]\n",
    "        \n",
    "        print(f\"\\nEliminando {len(to_delete)} versiones antiguas...\")\n",
    "        \n",
    "        for model_info in to_delete:\n",
    "            model_path = Path(model_info['path'])\n",
    "            if model_path.exists():\n",
    "                shutil.rmtree(model_path)\n",
    "                print(f\"  ‚úì Eliminado: {model_info['name']}\")\n",
    "        \n",
    "        print(f\"‚úì Limpieza completada, manteniendo {keep_versions} versiones\")\n",
    "    \n",
    "    def create_download_package(self, model_path, output_name=None):\n",
    "        \"\"\"\n",
    "        Crea un paquete comprimido para descarga\n",
    "        \"\"\"\n",
    "        model_path = Path(model_path)\n",
    "        \n",
    "        if output_name is None:\n",
    "            output_name = f\"{model_path.name}_package\"\n",
    "        \n",
    "        output_path = self.base_path / output_name\n",
    "        \n",
    "        print(f\"\\nCreando paquete de descarga...\")\n",
    "        print(f\"Origen: {model_path}\")\n",
    "        print(f\"Destino: {output_path}.tar.gz\")\n",
    "        \n",
    "        # Crear archivo tar.gz\n",
    "        import tarfile\n",
    "        \n",
    "        with tarfile.open(f\"{output_path}.tar.gz\", \"w:gz\") as tar:\n",
    "            tar.add(model_path, arcname=model_path.name)\n",
    "        \n",
    "        package_size = os.path.getsize(f\"{output_path}.tar.gz\") / (1024 * 1024)\n",
    "        \n",
    "        print(f\"‚úì Paquete creado: {output_path}.tar.gz\")\n",
    "        print(f\"  Tama√±o: {package_size:.2f} MB\")\n",
    "        \n",
    "        return f\"{output_path}.tar.gz\"\n",
    "\n",
    "# Inicializar manager\n",
    "model_manager = ModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo con m√©tricas\n",
    "model_path, metadata = model_manager.save_model(\n",
    "    model=model,\n",
    "    metrics=evaluator.metrics,\n",
    "    training_params=trainer.training_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7958147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar m√©tricas separadamente\n",
    "metrics_file = evaluator.save_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f95200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar modelos guardados\n",
    "print(\"\\nModelos guardados:\")\n",
    "print(\"=\"*80)\n",
    "saved_models = model_manager.list_models()\n",
    "\n",
    "for i, model_info in enumerate(saved_models, 1):\n",
    "    print(f\"\\n{i}. {model_info['name']}\")\n",
    "    print(f\"   Versi√≥n: {model_info['version']}\")\n",
    "    print(f\"   Fecha: {model_info['timestamp']}\")\n",
    "    if 'rmse' in model_info['metrics']:\n",
    "        print(f\"   RMSE: {model_info['metrics']['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f0e78",
   "metadata": {},
   "source": [
    "## 6. Sistema de Descarga del Modelo Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffec69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear paquete de descarga\n",
    "if saved_models:\n",
    "    latest_model = saved_models[-1]\n",
    "    package_path = model_manager.create_download_package(latest_model['path'])\n",
    "    \n",
    "    print(f\"\\nüì¶ Paquete listo para descarga:\")\n",
    "    print(f\"   {package_path}\")\n",
    "    print(f\"\\nüí° Para descargar desde Kaggle:\")\n",
    "    print(f\"   1. Ve a 'Output' en el panel derecho\")\n",
    "    print(f\"   2. Descarga el archivo .tar.gz\")\n",
    "else:\n",
    "    print(\"‚ö† No hay modelos guardados para empaquetar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalModelLoader:\n",
    "    \"\"\"\n",
    "    Carga y usa el modelo en el sistema local (fuera de Kaggle)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session=None):\n",
    "        self.spark = spark_session\n",
    "        self.model = None\n",
    "        self.metadata = None\n",
    "    \n",
    "    def extract_model_package(self, package_path, extract_to=\"./models\"):\n",
    "        \"\"\"\n",
    "        Extrae el paquete del modelo descargado de Kaggle\n",
    "        \"\"\"\n",
    "        import tarfile\n",
    "        \n",
    "        extract_path = Path(extract_to)\n",
    "        extract_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Extrayendo modelo desde: {package_path}\")\n",
    "        print(f\"Destino: {extract_path}\")\n",
    "        \n",
    "        with tarfile.open(package_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        \n",
    "        # Buscar el directorio del modelo\n",
    "        extracted_folders = list(extract_path.iterdir())\n",
    "        if extracted_folders:\n",
    "            model_dir = extracted_folders[0]\n",
    "            print(f\"‚úì Modelo extra√≠do en: {model_dir}\")\n",
    "            return model_dir\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No se encontr√≥ el modelo extra√≠do\")\n",
    "    \n",
    "    def load_from_path(self, model_path):\n",
    "        \"\"\"\n",
    "        Carga el modelo desde una ruta local\n",
    "        \"\"\"\n",
    "        if self.spark is None:\n",
    "            raise ValueError(\"Se requiere una SparkSession activa\")\n",
    "        \n",
    "        model_path = Path(model_path)\n",
    "        spark_model_path = model_path / \"spark_model\"\n",
    "        \n",
    "        print(f\"\\nCargando modelo local desde: {model_path}\")\n",
    "        \n",
    "        if not spark_model_path.exists():\n",
    "            raise FileNotFoundError(f\"No se encontr√≥ spark_model en: {spark_model_path}\")\n",
    "        \n",
    "        # Cargar modelo\n",
    "        self.model = ALSModel.load(str(spark_model_path))\n",
    "        print(f\"‚úì Modelo ALS cargado\")\n",
    "        \n",
    "        # Cargar metadatos\n",
    "        metadata_path = model_path / \"metadata.json\"\n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "            print(f\"‚úì Metadatos cargados\")\n",
    "            print(f\"   - Versi√≥n: {self.metadata.get('version', 'N/A')}\")\n",
    "            print(f\"   - Entrenado: {self.metadata.get('timestamp', 'N/A')}\")\n",
    "            if 'metrics' in self.metadata and 'rmse' in self.metadata['metrics']:\n",
    "                print(f\"   - RMSE: {self.metadata['metrics']['rmse']:.4f}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def recommend_for_user(self, user_id, num_recommendations=10):\n",
    "        \"\"\"\n",
    "        Genera recomendaciones para un usuario espec√≠fico\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Primero debes cargar un modelo\")\n",
    "        \n",
    "        # Crear DataFrame con el usuario\n",
    "        user_df = self.spark.createDataFrame([(user_id,)], [\"userId\"])\n",
    "        \n",
    "        # Generar recomendaciones\n",
    "        recommendations = self.model.recommendForUserSubset(user_df, num_recommendations)\n",
    "        \n",
    "        # Convertir a formato legible\n",
    "        recs_collected = recommendations.collect()\n",
    "        \n",
    "        if not recs_collected:\n",
    "            return []\n",
    "        \n",
    "        recommendations_list = []\n",
    "        for row in recs_collected:\n",
    "            for rec in row['recommendations']:\n",
    "                recommendations_list.append({\n",
    "                    'movieId': rec['movieId'],\n",
    "                    'score': float(rec['rating'])\n",
    "                })\n",
    "        \n",
    "        return recommendations_list\n",
    "    \n",
    "    def recommend_for_users(self, user_ids, num_recommendations=10):\n",
    "        \"\"\"\n",
    "        Genera recomendaciones para m√∫ltiples usuarios\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Primero debes cargar un modelo\")\n",
    "        \n",
    "        # Crear DataFrame con los usuarios\n",
    "        users_df = self.spark.createDataFrame([(uid,) for uid in user_ids], [\"userId\"])\n",
    "        \n",
    "        # Generar recomendaciones\n",
    "        recommendations = self.model.recommendForUserSubset(users_df, num_recommendations)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def predict_rating(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        Predice el rating de un usuario para una pel√≠cula espec√≠fica\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Primero debes cargar un modelo\")\n",
    "        \n",
    "        # Crear DataFrame con el par usuario-pel√≠cula\n",
    "        test_df = self.spark.createDataFrame(\n",
    "            [(user_id, movie_id)], \n",
    "            [\"userId\", \"movieId\"]\n",
    "        )\n",
    "        \n",
    "        # Predecir\n",
    "        prediction = self.model.transform(test_df)\n",
    "        \n",
    "        # Obtener resultado\n",
    "        result = prediction.select(\"prediction\").collect()\n",
    "        \n",
    "        if result and result[0]['prediction'] is not None:\n",
    "            return float(result[0]['prediction'])\n",
    "        else:\n",
    "            return None  # Cold start\n",
    "    \n",
    "    def batch_predict(self, user_movie_pairs):\n",
    "        \"\"\"\n",
    "        Predice ratings para m√∫ltiples pares usuario-pel√≠cula\n",
    "        \n",
    "        Args:\n",
    "            user_movie_pairs: Lista de tuplas (userId, movieId)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Primero debes cargar un modelo\")\n",
    "        \n",
    "        # Crear DataFrame\n",
    "        test_df = self.spark.createDataFrame(\n",
    "            user_movie_pairs, \n",
    "            [\"userId\", \"movieId\"]\n",
    "        )\n",
    "        \n",
    "        # Predecir\n",
    "        predictions = self.model.transform(test_df)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Ejemplo de uso en sistema local\n",
    "print(\"=\"*60)\n",
    "print(\"EJEMPLO DE USO EN SISTEMA LOCAL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "# 1. Crear SparkSession en tu sistema local\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_local = SparkSession.builder \\\\\n",
    "    .appName(\"ALS-Predictions\") \\\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Cargar el modelo descargado\n",
    "loader = LocalModelLoader(spark_local)\n",
    "model = loader.load_from_path(\"./models/als_model_v1_20251208_143022\")\n",
    "\n",
    "# 3. Generar recomendaciones\n",
    "recommendations = loader.recommend_for_user(user_id=123, num_recommendations=10)\n",
    "print(recommendations)\n",
    "\n",
    "# 4. Predecir rating espec√≠fico\n",
    "rating = loader.predict_rating(user_id=123, movie_id=456)\n",
    "print(f\"Rating predicho: {rating:.2f}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69363fd8",
   "metadata": {},
   "source": [
    "## 8. Reentrenamiento Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalTrainer:\n",
    "    \"\"\"\n",
    "    Maneja el reentrenamiento incremental del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark, model_manager):\n",
    "        self.spark = spark\n",
    "        self.model_manager = model_manager\n",
    "        self.current_model = None\n",
    "        self.training_history = []\n",
    "    \n",
    "    def check_retraining_needed(self, new_ratings_df, last_training_size):\n",
    "        \"\"\"\n",
    "        Determina si se necesita reentrenamiento basado en nuevos datos\n",
    "        \"\"\"\n",
    "        new_count = new_ratings_df.count()\n",
    "        \n",
    "        if last_training_size == 0:\n",
    "            return True, \"No hay modelo entrenado\"\n",
    "        \n",
    "        growth_ratio = new_count / last_training_size\n",
    "        \n",
    "        if growth_ratio >= Config.INCREMENTAL_THRESHOLD:\n",
    "            return True, f\"Crecimiento de datos: {growth_ratio:.1%}\"\n",
    "        \n",
    "        return False, f\"Crecimiento insuficiente: {growth_ratio:.1%}\"\n",
    "    \n",
    "    def incremental_retrain(self, existing_model, new_ratings_df, full_ratings_df=None):\n",
    "        \"\"\"\n",
    "        Realiza reentrenamiento incremental\n",
    "        \n",
    "        Estrategia:\n",
    "        1. Si hay pocos datos nuevos (< 10%): solo actualizar factores\n",
    "        2. Si hay muchos datos nuevos (>= 10%): reentrenar completo\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"REENTRENAMIENTO INCREMENTAL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        new_count = new_ratings_df.count()\n",
    "        \n",
    "        if full_ratings_df is None:\n",
    "            full_ratings_df = new_ratings_df\n",
    "        \n",
    "        total_count = full_ratings_df.count()\n",
    "        \n",
    "        print(f\"\\nNuevos ratings: {new_count:,}\")\n",
    "        print(f\"Total ratings: {total_count:,}\")\n",
    "        print(f\"Proporci√≥n nueva: {new_count/total_count:.1%}\")\n",
    "        \n",
    "        # Decidir estrategia\n",
    "        if new_count / total_count < Config.INCREMENTAL_THRESHOLD:\n",
    "            print(\"\\n‚Üí Estrategia: Actualizaci√≥n r√°pida (pocos datos nuevos)\")\n",
    "            return self._quick_update(existing_model, new_ratings_df, full_ratings_df)\n",
    "        else:\n",
    "            print(\"\\n‚Üí Estrategia: Reentrenamiento completo (muchos datos nuevos)\")\n",
    "            return self._full_retrain(full_ratings_df)\n",
    "    \n",
    "    def _quick_update(self, existing_model, new_ratings_df, full_ratings_df):\n",
    "        \"\"\"\n",
    "        Actualizaci√≥n r√°pida para pocos datos nuevos\n",
    "        \n",
    "        Nota: Spark ALS no soporta warm-start nativo,\n",
    "        pero podemos re-entrenar con menos iteraciones\n",
    "        \"\"\"\n",
    "        print(\"\\nReentrenando con par√°metros reducidos...\")\n",
    "        \n",
    "        # Obtener par√°metros del modelo existente\n",
    "        rank = existing_model.rank\n",
    "        \n",
    "        # Re-entrenar con menos iteraciones\n",
    "        als = ALS(\n",
    "            rank=rank,\n",
    "            maxIter=3,  # Pocas iteraciones para actualizaci√≥n r√°pida\n",
    "            regParam=Config.DEFAULT_REG_PARAM,\n",
    "            userCol=\"userId\",\n",
    "            itemCol=\"movieId\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        new_model = als.fit(full_ratings_df)\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"‚úì Actualizaci√≥n completada en {duration:.2f} segundos\")\n",
    "        \n",
    "        return new_model, {\n",
    "            'strategy': 'quick_update',\n",
    "            'duration': duration,\n",
    "            'iterations': 3\n",
    "        }\n",
    "    \n",
    "    def _full_retrain(self, full_ratings_df):\n",
    "        \"\"\"\n",
    "        Reentrenamiento completo con todos los datos\n",
    "        \"\"\"\n",
    "        print(\"\\nReentrenando modelo completo...\")\n",
    "        \n",
    "        als = ALS(\n",
    "            rank=Config.DEFAULT_RANK,\n",
    "            maxIter=Config.DEFAULT_MAX_ITER,\n",
    "            regParam=Config.DEFAULT_REG_PARAM,\n",
    "            userCol=\"userId\",\n",
    "            itemCol=\"movieId\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True,\n",
    "            seed=42,\n",
    "            checkpointInterval=10\n",
    "        )\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        new_model = als.fit(full_ratings_df)\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"‚úì Reentrenamiento completo en {duration:.2f} segundos\")\n",
    "        \n",
    "        return new_model, {\n",
    "            'strategy': 'full_retrain',\n",
    "            'duration': duration,\n",
    "            'iterations': Config.DEFAULT_MAX_ITER\n",
    "        }\n",
    "    \n",
    "    def auto_retrain_pipeline(self, ratings_df, force_retrain=False):\n",
    "        \"\"\"\n",
    "        Pipeline autom√°tico de reentrenamiento\n",
    "        \n",
    "        Flujo:\n",
    "        1. Buscar modelo existente\n",
    "        2. Si no existe, entrenar nuevo modelo (quick)\n",
    "        3. Si existe, evaluar si reentrenar\n",
    "        4. Guardar nuevo modelo si mejora\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE AUTOM√ÅTICO DE REENTRENAMIENTO\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Buscar modelo existente\n",
    "        existing_models = self.model_manager.list_models()\n",
    "        \n",
    "        if not existing_models and not force_retrain:\n",
    "            print(\"\\n‚Üí No se encontr√≥ modelo existente\")\n",
    "            print(\"‚Üí Entrenando modelo inicial con par√°metros optimizados...\")\n",
    "            \n",
    "            # Entrenar modelo inicial r√°pido\n",
    "            als = ALS(\n",
    "                rank=Config.DEFAULT_RANK,\n",
    "                maxIter=Config.DEFAULT_MAX_ITER,\n",
    "                regParam=Config.DEFAULT_REG_PARAM,\n",
    "                userCol=\"userId\",\n",
    "                itemCol=\"movieId\",\n",
    "                ratingCol=\"rating\",\n",
    "                coldStartStrategy=\"drop\",\n",
    "                nonnegative=True,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            new_model = als.fit(ratings_df)\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            print(f\"‚úì Modelo inicial entrenado en {duration:.2f} segundos\")\n",
    "            \n",
    "            # Evaluar\n",
    "            train_temp, test_temp = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "            evaluator_temp = ModelEvaluator(new_model, self.spark)\n",
    "            metrics_temp = evaluator_temp.evaluate_regression_metrics(test_temp)\n",
    "            \n",
    "            # Guardar\n",
    "            self.model_manager.save_model(\n",
    "                model=new_model,\n",
    "                metrics=metrics_temp,\n",
    "                training_params={\n",
    "                    'strategy': 'initial_training',\n",
    "                    'duration': duration,\n",
    "                    'rank': Config.DEFAULT_RANK,\n",
    "                    'max_iter': Config.DEFAULT_MAX_ITER\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.current_model = new_model\n",
    "            \n",
    "            return new_model, metrics_temp, 'initial_training'\n",
    "        \n",
    "        elif not existing_models and force_retrain:\n",
    "            print(\"\\n‚Üí Forzando entrenamiento inicial...\")\n",
    "            # Mismo flujo que arriba\n",
    "            return self.auto_retrain_pipeline(ratings_df, force_retrain=False)\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\n‚Üí Encontrados {len(existing_models)} modelos existentes\")\n",
    "            \n",
    "            # Cargar modelo m√°s reciente\n",
    "            latest_model_info = existing_models[-1]\n",
    "            print(f\"‚Üí Cargando modelo: {latest_model_info['name']}\")\n",
    "            \n",
    "            existing_model, metadata = self.model_manager.load_model(\n",
    "                latest_model_info['path']\n",
    "            )\n",
    "            \n",
    "            last_training_size = metadata.get('training_params', {}).get('train_size', 0)\n",
    "            \n",
    "            # Verificar si se necesita reentrenamiento\n",
    "            current_size = ratings_df.count()\n",
    "            needs_retrain, reason = self.check_retraining_needed(\n",
    "                ratings_df, \n",
    "                last_training_size\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚Üí Evaluaci√≥n de reentrenamiento:\")\n",
    "            print(f\"   Raz√≥n: {reason}\")\n",
    "            print(f\"   ¬øReentrenar?: {'S√≠' if needs_retrain else 'No'}\")\n",
    "            \n",
    "            if not needs_retrain and not force_retrain:\n",
    "                print(\"\\n‚úì No se requiere reentrenamiento\")\n",
    "                self.current_model = existing_model\n",
    "                return existing_model, metadata.get('metrics', {}), 'no_retrain'\n",
    "            \n",
    "            # Realizar reentrenamiento\n",
    "            print(\"\\n‚Üí Iniciando reentrenamiento...\")\n",
    "            \n",
    "            new_model, retrain_info = self._full_retrain(ratings_df)\n",
    "            \n",
    "            # Evaluar nuevo modelo\n",
    "            train_temp, test_temp = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "            evaluator_temp = ModelEvaluator(new_model, self.spark)\n",
    "            new_metrics = evaluator_temp.evaluate_regression_metrics(test_temp)\n",
    "            \n",
    "            # Comparar con modelo anterior\n",
    "            old_rmse = metadata.get('metrics', {}).get('rmse', float('inf'))\n",
    "            new_rmse = new_metrics.get('rmse', float('inf'))\n",
    "            \n",
    "            print(f\"\\n‚Üí Comparaci√≥n de m√©tricas:\")\n",
    "            print(f\"   RMSE anterior: {old_rmse:.4f}\")\n",
    "            print(f\"   RMSE nuevo: {new_rmse:.4f}\")\n",
    "            print(f\"   Mejora: {((old_rmse - new_rmse) / old_rmse * 100):.2f}%\")\n",
    "            \n",
    "            if new_rmse <= old_rmse or force_retrain:\n",
    "                print(\"\\n‚úì Guardando nuevo modelo (mejor o forzado)...\")\n",
    "                \n",
    "                self.model_manager.save_model(\n",
    "                    model=new_model,\n",
    "                    metrics=new_metrics,\n",
    "                    training_params={\n",
    "                        **retrain_info,\n",
    "                        'train_size': current_size,\n",
    "                        'previous_rmse': old_rmse\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Limpiar versiones antiguas\n",
    "                self.model_manager.cleanup_old_versions()\n",
    "                \n",
    "                self.current_model = new_model\n",
    "                \n",
    "                return new_model, new_metrics, 'retrained'\n",
    "            else:\n",
    "                print(\"\\n‚ö† Modelo nuevo no mejora, manteniendo modelo anterior\")\n",
    "                self.current_model = existing_model\n",
    "                return existing_model, metadata.get('metrics', {}), 'kept_old'\n",
    "\n",
    "# Inicializar trainer incremental\n",
    "incremental_trainer = IncrementalTrainer(spark, model_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1f887",
   "metadata": {},
   "source": [
    "## 9. Sistema de Actualizaci√≥n Autom√°tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoUpdateSystem:\n",
    "    \"\"\"\n",
    "    Sistema de actualizaci√≥n autom√°tica para producci√≥n\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark, model_manager, incremental_trainer):\n",
    "        self.spark = spark\n",
    "        self.model_manager = model_manager\n",
    "        self.incremental_trainer = incremental_trainer\n",
    "        self.update_log = []\n",
    "    \n",
    "    def schedule_info(self):\n",
    "        \"\"\"\n",
    "        Informaci√≥n sobre c√≥mo programar actualizaciones autom√°ticas\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"PROGRAMACI√ìN DE ACTUALIZACIONES AUTOM√ÅTICAS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\"\"\n",
    "### Opci√≥n 1: Cron Job (Linux/Mac)\n",
    "\n",
    "Crear script: /home/user/als_training/update_model.sh\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#!/bin/bash\n",
    "cd /home/user/als_training\n",
    "source venv/bin/activate\n",
    "python run_update.py >> logs/update_$(date +%Y%m%d).log 2>&1\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "Agregar a crontab:\n",
    "# Actualizar modelo cada d√≠a a las 2 AM\n",
    "0 2 * * * /home/user/als_training/update_model.sh\n",
    "\n",
    "# Actualizar modelo cada semana (domingo 3 AM)\n",
    "0 3 * * 0 /home/user/als_training/update_model.sh\n",
    "\n",
    "\n",
    "### Opci√≥n 2: Kaggle Notebooks Schedule\n",
    "\n",
    "1. En tu notebook de Kaggle, habilita \"Schedule Run\"\n",
    "2. Configura frecuencia: Daily, Weekly, etc.\n",
    "3. El notebook se ejecutar√° autom√°ticamente\n",
    "\n",
    "\n",
    "### Opci√≥n 3: Apache Airflow DAG\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "dag = DAG(\n",
    "    'als_model_update',\n",
    "    default_args={\n",
    "        'owner': 'data-team',\n",
    "        'retries': 2,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "    },\n",
    "    schedule_interval='0 2 * * *',  # Diario 2 AM\n",
    "    start_date=datetime(2025, 1, 1),\n",
    ")\n",
    "\n",
    "update_task = BashOperator(\n",
    "    task_id='update_als_model',\n",
    "    bash_command='python /path/to/run_update.py',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "### Opci√≥n 4: Cloud Functions (AWS Lambda / GCP)\n",
    "\n",
    "Configurar trigger schedule con CloudWatch Events o Cloud Scheduler\n",
    "        \"\"\")\n",
    "    \n",
    "    def create_update_script(self, output_path=\"./run_update.py\"):\n",
    "        \"\"\"\n",
    "        Crea script Python para actualizaciones autom√°ticas\n",
    "        \"\"\"\n",
    "        script_content = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script de actualizaci√≥n autom√°tica del modelo ALS\n",
    "Ejecutar con: python run_update.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "\n",
    "# Importar tus clases (ajustar rutas seg√∫n tu proyecto)\n",
    "# from model_manager import ModelManager, IncrementalTrainer, DataLoader\n",
    "\n",
    "def main():\n",
    "    print(f\"Inicio de actualizaci√≥n: {datetime.now()}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Crear SparkSession\n",
    "        spark = SparkSession.builder \\\\\n",
    "            .appName(\"ALS-Auto-Update\") \\\\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        print(\"‚úì Spark iniciado\")\n",
    "        \n",
    "        # 2. Cargar datos nuevos\n",
    "        # Ajustar seg√∫n tu fuente de datos\n",
    "        ratings_path = \"/path/to/ratings/data\"\n",
    "        ratings_df = spark.read.parquet(ratings_path)\n",
    "        \n",
    "        print(f\"‚úì Datos cargados: {ratings_df.count():,} registros\")\n",
    "        \n",
    "        # 3. Inicializar sistema\n",
    "        model_manager = ModelManager()\n",
    "        incremental_trainer = IncrementalTrainer(spark, model_manager)\n",
    "        \n",
    "        # 4. Ejecutar pipeline de reentrenamiento\n",
    "        new_model, metrics, action = incremental_trainer.auto_retrain_pipeline(\n",
    "            ratings_df=ratings_df,\n",
    "            force_retrain=False\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Pipeline completado: {action}\")\n",
    "        print(f\"  RMSE: {metrics.get('rmse', 'N/A')}\")\n",
    "        \n",
    "        # 5. Crear paquete de descarga si hay nuevo modelo\n",
    "        if action in ['initial_training', 'retrained']:\n",
    "            models = model_manager.list_models()\n",
    "            if models:\n",
    "                latest = models[-1]\n",
    "                package = model_manager.create_download_package(latest['path'])\n",
    "                print(f\"‚úì Paquete creado: {package}\")\n",
    "        \n",
    "        spark.stop()\n",
    "        \n",
    "        print(f\"‚úì Actualizaci√≥n completada: {datetime.now()}\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error durante actualizaci√≥n: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''\n",
    "        \n",
    "        output_path = Path(output_path)\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        # Hacer ejecutable\n",
    "        import os\n",
    "        os.chmod(output_path, 0o755)\n",
    "        \n",
    "        print(f\"‚úì Script de actualizaci√≥n creado: {output_path}\")\n",
    "        print(f\"  Ejecutar con: python {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def simulate_streaming_update(self, base_ratings_df, new_data_ratio=0.05):\n",
    "        \"\"\"\n",
    "        Simula actualizaci√≥n con nuevos datos (como streaming)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SIMULACI√ìN DE ACTUALIZACI√ìN CON NUEVOS DATOS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_count = base_ratings_df.count()\n",
    "        \n",
    "        # Simular \"nuevos datos\" tomando una muestra\n",
    "        new_sample_size = int(total_count * new_data_ratio)\n",
    "        \n",
    "        print(f\"\\nSimulando {new_sample_size:,} nuevos ratings ({new_data_ratio:.1%})\")\n",
    "        \n",
    "        # En producci√≥n, esto ser√≠an datos reales nuevos\n",
    "        new_ratings = base_ratings_df.sample(fraction=new_data_ratio, seed=123)\n",
    "        \n",
    "        # Combinar con datos existentes\n",
    "        combined_df = base_ratings_df.union(new_ratings).distinct()\n",
    "        \n",
    "        print(f\"Total despu√©s de nuevos datos: {combined_df.count():,}\")\n",
    "        \n",
    "        # Ejecutar pipeline de actualizaci√≥n\n",
    "        updated_model, metrics, action = self.incremental_trainer.auto_retrain_pipeline(\n",
    "            ratings_df=combined_df,\n",
    "            force_retrain=False\n",
    "        )\n",
    "        \n",
    "        # Registrar actualizaci√≥n\n",
    "        self.update_log.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': action,\n",
    "            'new_data_ratio': new_data_ratio,\n",
    "            'metrics': metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚úì Simulaci√≥n completada\")\n",
    "        print(f\"  Acci√≥n tomada: {action}\")\n",
    "        \n",
    "        return updated_model, metrics\n",
    "    \n",
    "    def get_update_history(self):\n",
    "        \"\"\"\n",
    "        Obtiene historial de actualizaciones\n",
    "        \"\"\"\n",
    "        if not self.update_log:\n",
    "            print(\"No hay historial de actualizaciones\")\n",
    "            return []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"HISTORIAL DE ACTUALIZACIONES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, entry in enumerate(self.update_log, 1):\n",
    "            print(f\"\\n{i}. {entry['timestamp']}\")\n",
    "            print(f\"   Acci√≥n: {entry['action']}\")\n",
    "            print(f\"   Datos nuevos: {entry['new_data_ratio']:.1%}\")\n",
    "            if 'rmse' in entry['metrics']:\n",
    "                print(f\"   RMSE: {entry['metrics']['rmse']:.4f}\")\n",
    "        \n",
    "        return self.update_log\n",
    "    \n",
    "    def health_check(self):\n",
    "        \"\"\"\n",
    "        Verifica el estado del sistema\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"HEALTH CHECK DEL SISTEMA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        checks = []\n",
    "        \n",
    "        # 1. Verificar modelos guardados\n",
    "        models = self.model_manager.list_models()\n",
    "        if models:\n",
    "            checks.append((\"‚úì\", f\"Modelos guardados: {len(models)}\"))\n",
    "            latest = models[-1]\n",
    "            checks.append((\"‚úì\", f\"√öltimo modelo: {latest['name']}\"))\n",
    "        else:\n",
    "            checks.append((\"‚úó\", \"No hay modelos guardados\"))\n",
    "        \n",
    "        # 2. Verificar Spark\n",
    "        try:\n",
    "            spark_version = self.spark.version\n",
    "            checks.append((\"‚úì\", f\"Spark activo: v{spark_version}\"))\n",
    "        except:\n",
    "            checks.append((\"‚úó\", \"Spark no disponible\"))\n",
    "        \n",
    "        # 3. Verificar directorios\n",
    "        for path_name, path in [\n",
    "            (\"Modelos\", Config.MODEL_PATH),\n",
    "            (\"M√©tricas\", Config.METRICS_PATH),\n",
    "            (\"Logs\", Config.LOGS_PATH)\n",
    "        ]:\n",
    "            if path.exists():\n",
    "                checks.append((\"‚úì\", f\"Directorio {path_name}: OK\"))\n",
    "            else:\n",
    "                checks.append((\"‚úó\", f\"Directorio {path_name}: No existe\"))\n",
    "        \n",
    "        # Imprimir resultados\n",
    "        for status, message in checks:\n",
    "            print(f\"  {status} {message}\")\n",
    "        \n",
    "        # Resumen\n",
    "        passed = sum(1 for s, _ in checks if s == \"‚úì\")\n",
    "        total = len(checks)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTADO: {passed}/{total} checks pasados\")\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"‚úì Sistema saludable\")\n",
    "        elif passed >= total * 0.7:\n",
    "            print(\"‚ö† Sistema funcional con advertencias\")\n",
    "        else:\n",
    "            print(\"‚úó Sistema requiere atenci√≥n\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return passed == total\n",
    "\n",
    "# Inicializar sistema de actualizaci√≥n\n",
    "auto_update = AutoUpdateSystem(spark, model_manager, incremental_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e55b8d",
   "metadata": {},
   "source": [
    "## 10. Validaci√≥n y Monitoreo del Sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fd02b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Gu√≠a de Uso Completa\n",
    "\n",
    "### üìù Flujo de Trabajo Recomendado\n",
    "\n",
    "#### **En Kaggle (Entrenamiento):**\n",
    "\n",
    "1. **Primera Ejecuci√≥n:**\n",
    "   - Ejecuta todas las celdas secuencialmente\n",
    "   - El sistema detecta que no hay modelo y entrena uno nuevo (rank=10, iter=5)\n",
    "   - Tiempo estimado: ~5-8 minutos\n",
    "   \n",
    "2. **Ejecuciones Posteriores:**\n",
    "   - El sistema detecta modelo existente\n",
    "   - Eval√∫a si se necesita reentrenamiento\n",
    "   - Solo reentrena si hay suficientes datos nuevos\n",
    "\n",
    "3. **Descarga:**\n",
    "   - Ve a Output ‚Üí descarga el `.tar.gz`\n",
    "\n",
    "---\n",
    "\n",
    "#### **En Sistema Local (Predicci√≥n):**\n",
    "\n",
    "```python\n",
    "# 1. Instalar dependencias\n",
    "pip install pyspark==3.5.0\n",
    "\n",
    "# 2. Extraer modelo\n",
    "tar -xzf als_model_v1_*.tar.gz -C ./models/\n",
    "\n",
    "# 3. Usar modelo\n",
    "from pyspark.sql import SparkSession\n",
    "from local_model_loader import LocalModelLoader\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS-Predictions\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "loader = LocalModelLoader(spark)\n",
    "model = loader.load_from_path(\"./models/als_model_v1_...\")\n",
    "\n",
    "# Recomendar para usuario\n",
    "recs = loader.recommend_for_user(user_id=123, num_recommendations=10)\n",
    "print(recs)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Automatizaci√≥n en Producci√≥n\n",
    "\n",
    "#### **Opci√≥n 1: Script Python Standalone**\n",
    "\n",
    "```bash\n",
    "# Ejecutar manualmente\n",
    "python run_update.py\n",
    "\n",
    "# O programar con cron\n",
    "0 2 * * * /path/to/run_update.py >> logs/update.log 2>&1\n",
    "```\n",
    "\n",
    "#### **Opci√≥n 2: Kaggle Schedule**\n",
    "- Configura \"Schedule Run\" en Kaggle\n",
    "- El notebook se ejecuta autom√°ticamente\n",
    "- Descarga modelos v√≠a API de Kaggle\n",
    "\n",
    "#### **Opci√≥n 3: Integraci√≥n con tu Sistema**\n",
    "\n",
    "```python\n",
    "# En tu sistema Spark existente\n",
    "from incremental_trainer import IncrementalTrainer\n",
    "from model_manager import ModelManager\n",
    "\n",
    "# Cargar ratings desde HDFS\n",
    "ratings_df = spark.read.parquet(\"hdfs://namenode:9000/streams/ratings/raw\")\n",
    "\n",
    "# Ejecutar pipeline\n",
    "manager = ModelManager(\"hdfs://namenode:9000/models/als\")\n",
    "trainer = IncrementalTrainer(spark, manager)\n",
    "\n",
    "model, metrics, action = trainer.auto_retrain_pipeline(ratings_df)\n",
    "\n",
    "# Servir recomendaciones\n",
    "recommendations = model.recommendForAllUsers(10)\n",
    "recommendations.write.parquet(\"hdfs://namenode:9000/recommendations/latest\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Monitoreo de M√©tricas\n",
    "\n",
    "**M√©tricas Clave:**\n",
    "- **RMSE < 0.90**: Excelente\n",
    "- **RMSE 0.90-1.00**: Bueno\n",
    "- **RMSE > 1.00**: Requiere optimizaci√≥n\n",
    "\n",
    "**Precision@10 > 0.15**: Buen rendimiento para MovieLens\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Troubleshooting\n",
    "\n",
    "| Problema | Soluci√≥n |\n",
    "|----------|----------|\n",
    "| `OutOfMemoryError` | Reducir `rank` a 5-10 o `spark.driver.memory` |\n",
    "| Entrenamiento muy lento | Reducir `maxIter` a 3-5 |\n",
    "| Cold start en predicciones | Usar `coldStartStrategy=\"drop\"` |\n",
    "| Modelo no mejora | Aumentar datos de entrenamiento o ajustar `regParam` |\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Estructura de Archivos Generados\n",
    "\n",
    "```\n",
    "/kaggle/working/\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ als_model_v1_20251208_143022/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ spark_model/           # Modelo Spark\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ metadata.json           # Metadatos y m√©tricas\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ model_info.json         # Info de factores\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ checksum.txt            # Validaci√≥n\n",
    "‚îú‚îÄ‚îÄ metrics/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metrics_20251208_143022.json\n",
    "‚îú‚îÄ‚îÄ logs/\n",
    "‚îî‚îÄ‚îÄ run_update.py                   # Script de actualizaci√≥n\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Escalabilidad y Mantenimiento\n",
    "\n",
    "**Dise√±o Escalable:**\n",
    "- ‚úÖ Versionado autom√°tico de modelos\n",
    "- ‚úÖ Limpieza de versiones antiguas\n",
    "- ‚úÖ Checkpoints para fault tolerance\n",
    "- ‚úÖ Actualizaci√≥n incremental inteligente\n",
    "- ‚úÖ M√©tricas hist√≥ricas\n",
    "\n",
    "**Mantenimiento:**\n",
    "- Revisa logs de actualizaci√≥n regularmente\n",
    "- Monitorea degradaci√≥n de m√©tricas\n",
    "- Ajusta `INCREMENTAL_THRESHOLD` seg√∫n necesidad\n",
    "- Mant√©n m√°ximo 5 versiones (configurable)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Sistema Listo para Producci√≥n!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dad2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final del sistema\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FINAL DEL SISTEMA ALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Estad√≠sticas de Datos:\")\n",
    "print(f\"  - Total de ratings: {stats.get('total_ratings', 'N/A'):,}\")\n",
    "print(f\"  - Usuarios √∫nicos: {stats.get('unique_users', 'N/A'):,}\")\n",
    "print(f\"  - Pel√≠culas √∫nicas: {stats.get('unique_movies', 'N/A'):,}\")\n",
    "print(f\"  - Sparsity: {stats.get('sparsity', 0):.2%}\")\n",
    "\n",
    "print(\"\\nüéØ Modelo Activo:\")\n",
    "models = model_manager.list_models()\n",
    "if models:\n",
    "    latest = models[-1]\n",
    "    print(f\"  - Versi√≥n: {latest['version']}\")\n",
    "    print(f\"  - Nombre: {latest['name']}\")\n",
    "    print(f\"  - Fecha: {latest['timestamp']}\")\n",
    "    if 'rmse' in latest['metrics']:\n",
    "        print(f\"  - RMSE: {latest['metrics']['rmse']:.4f}\")\n",
    "    if 'mae' in latest['metrics']:\n",
    "        print(f\"  - MAE: {latest['metrics']['mae']:.4f}\")\n",
    "else:\n",
    "    print(\"  ‚ö† No hay modelos guardados\")\n",
    "\n",
    "print(\"\\nüì¶ Archivos Generados:\")\n",
    "print(f\"  - Directorio de modelos: {Config.MODEL_PATH}\")\n",
    "print(f\"  - Directorio de m√©tricas: {Config.METRICS_PATH}\")\n",
    "print(f\"  - Directorio de logs: {Config.LOGS_PATH}\")\n",
    "\n",
    "print(\"\\nüöÄ Pr√≥ximos Pasos:\")\n",
    "print(\"\"\"\n",
    "1. DESCARGA DEL MODELO:\n",
    "   - Ve a 'Output' en el panel derecho de Kaggle\n",
    "   - Descarga el archivo .tar.gz del modelo\n",
    "   \n",
    "2. USO EN SISTEMA LOCAL:\n",
    "   - Extrae el modelo: tar -xzf als_model_*.tar.gz\n",
    "   - Carga con LocalModelLoader\n",
    "   - Genera recomendaciones\n",
    "   \n",
    "3. PRODUCCI√ìN:\n",
    "   - Configura actualizaci√≥n autom√°tica (cron/Airflow)\n",
    "   - Implementa API REST para servir recomendaciones\n",
    "   - Integra con sistema de cache (Redis)\n",
    "   - Monitoreo continuo de m√©tricas\n",
    "   \n",
    "4. INTEGRACI√ìN CON TU SISTEMA:\n",
    "   - Copia el modelo a: /home/abraham/Escritorio/PGVD/.../movies/models/\n",
    "   - Crea servicio de recomendaciones\n",
    "   - Conecta con Kafka para actualizaciones en tiempo real\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular actualizaci√≥n con nuevos datos (opcional)\n",
    "# Descomenta para probar el flujo de actualizaci√≥n incremental\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"SIMULACI√ìN DE ACTUALIZACI√ìN INCREMENTAL\")\n",
    "# print(\"=\"*80)\n",
    "# \n",
    "# updated_model, updated_metrics = auto_update.simulate_streaming_update(\n",
    "#     base_ratings_df=ratings_df,\n",
    "#     new_data_ratio=0.05  # 5% de nuevos datos\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a500afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar health check del sistema\n",
    "system_healthy = auto_update.health_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar informaci√≥n de programaci√≥n\n",
    "auto_update.schedule_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear script de actualizaci√≥n autom√°tica\n",
    "update_script = auto_update.create_update_script(output_path=Config.BASE_PATH / \"run_update.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f279218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar pipeline autom√°tico de reentrenamiento\n",
    "# Este es el punto de entrada principal del sistema\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EJECUTANDO PIPELINE AUTOM√ÅTICO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Este pipeline:\n",
    "1. Verifica si existe un modelo entrenado\n",
    "2. Si NO existe: entrena uno nuevo con rank=10, iter=5 (r√°pido)\n",
    "3. Si existe: eval√∫a si necesita reentrenamiento\n",
    "4. Guarda el mejor modelo autom√°ticamente\n",
    "\"\"\")\n",
    "\n",
    "# Ejecutar pipeline\n",
    "final_model, final_metrics, action_taken = incremental_trainer.auto_retrain_pipeline(\n",
    "    ratings_df=ratings_df,\n",
    "    force_retrain=False  # Cambiar a True para forzar reentrenamiento\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RESULTADO DEL PIPELINE: {action_taken.upper()}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if final_metrics:\n",
    "    print(f\"\\nM√©tricas del modelo activo:\")\n",
    "    if 'rmse' in final_metrics:\n",
    "        print(f\"  - RMSE: {final_metrics['rmse']:.4f}\")\n",
    "    if 'mae' in final_metrics:\n",
    "        print(f\"  - MAE: {final_metrics['mae']:.4f}\")\n",
    "    if 'r2' in final_metrics:\n",
    "        print(f\"  - R¬≤: {final_metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fa28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n de predicci√≥n con el modelo actual\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMOSTRACI√ìN DE PREDICCIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Seleccionar usuarios de ejemplo\n",
    "sample_users = train_df.select(\"userId\").distinct().limit(3).collect()\n",
    "user_ids = [row['userId'] for row in sample_users]\n",
    "\n",
    "print(f\"\\nUsuarios de ejemplo: {user_ids}\")\n",
    "\n",
    "# Generar recomendaciones para cada usuario\n",
    "for user_id in user_ids:\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"Recomendaciones para Usuario {user_id}:\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    \n",
    "    # Crear DataFrame con el usuario\n",
    "    user_df = spark.createDataFrame([(user_id,)], [\"userId\"])\n",
    "    \n",
    "    # Generar top-5 recomendaciones\n",
    "    recs = model.recommendForUserSubset(user_df, 5)\n",
    "    recs_list = recs.collect()\n",
    "    \n",
    "    if recs_list:\n",
    "        for i, rec in enumerate(recs_list[0]['recommendations'], 1):\n",
    "            movie_id = rec['movieId']\n",
    "            score = rec['rating']\n",
    "            \n",
    "            # Buscar t√≠tulo de la pel√≠cula si est√° disponible\n",
    "            if movies_df is not None:\n",
    "                movie_info = movies_df.filter(F.col(\"movieId\") == movie_id).select(\"title\").collect()\n",
    "                title = movie_info[0]['title'] if movie_info else f\"Movie {movie_id}\"\n",
    "            else:\n",
    "                title = f\"Movie {movie_id}\"\n",
    "            \n",
    "            print(f\"  {i}. {title} (Score: {score:.2f})\")\n",
    "    else:\n",
    "        print(\"  No hay recomendaciones disponibles\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666102d2",
   "metadata": {},
   "source": [
    "## 7. Carga del Modelo para Predicciones Locales"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
