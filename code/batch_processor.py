from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import os

def setup_spark():
    """Configura Spark Session para HDFS"""
    return SparkSession.builder \
        .appName("MovieBatchAnalysis") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

def analyze_movie_data():
    """An√°lisis batch de datos de pel√≠culas"""
    print("üé¨ Iniciando an√°lisis batch...")
    
    spark = setup_spark()
    
    try:
        # Leer datos de HDFS
        print("üìñ Leyendo datos de HDFS...")
        df = spark.read.json("hdfs://namenode:9000/data/*.json")
        
        if df.count() == 0:
            print("‚ùå No hay datos en HDFS")
            return
        
        print(f"üìä Total de registros: {df.count()}")
        
        # An√°lisis b√°sico
        print("\nüìà AN√ÅLISIS BATCH:")
        print("1. Conteo por acci√≥n:")
        df.groupBy("action").count().show()
        
        print("2. Top 10 pel√≠culas m√°s interactivas:")
        df.groupBy("movie_id").count() \
          .orderBy(desc("count")) \
          .limit(10) \
          .show()
        
        print("3. Distribuci√≥n temporal:")
        df.withColumn("hour", hour(from_unixtime("timestamp"))) \
          .groupBy("hour").count() \
          .orderBy("hour") \
          .show()
        
        # Guardar resultados en HDFS
        results = df.groupBy("movie_id").agg(
            count("*").alias("total_interactions"),
            count(when(col("action") == "view", True)).alias("views"),
            count(when(col("action") == "rating", True)).alias("ratings")
        )
        
        results.write.mode("overwrite").json("hdfs://namenode:9000/results/movie_analytics")
        print("‚úÖ Resultados guardados en HDFS: /results/movie_analytics")
        
        # Mostrar estad√≠sticas finales
        print(f"\nüéØ ESTAD√çSTICAS FINALES:")
        print(f"   - Total interacciones: {df.count()}")
        print(f"   - Pel√≠culas √∫nicas: {df.select('movie_id').distinct().count()}")
        print(f"   - Acciones √∫nicas: {[row[0] for row in df.select('action').distinct().collect()]}")
        
    except Exception as e:
        print(f"‚ùå Error en an√°lisis batch: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    analyze_movie_data()