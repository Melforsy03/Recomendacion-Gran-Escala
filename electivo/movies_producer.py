from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json
import subprocess

print("üé¨ MOVIES PRODUCER CON YARN, SPARK Y HDFS")

# Configurar Spark para YARN
spark = SparkSession.builder \
    .appName("MoviesProducerYARN") \
    .master("yarn") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.executor.memory", "512m") \
    .config("spark.driver.memory", "512m") \
    .getOrCreate()

# Esquema de datos
movies_schema = StructType([
    StructField("ID", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("puan", DoubleType(), True),
    StructField("genre_1", StringType(), True),
    StructField("genre_2", StringType(), True),
    StructField("pop", IntegerType(), True),
    StructField("description", StringType(), True)
])

def verificar_servicios():
    """Verificar que HDFS y YARN est√©n funcionando"""
    try:
        # Verificar HDFS
        result = subprocess.run(["hdfs", "dfs", "-ls", "/"], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode != 0:
            print("‚ùå HDFS no est√° disponible")
            return False
        
        # Verificar YARN
        result = subprocess.run(["yarn", "node", "-list"], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode != 0:
            print("‚ùå YARN no est√° disponible")
            return False
            
        print("‚úÖ HDFS y YARN funcionando correctamente")
        return True
        
    except Exception as e:
        print(f"‚ùå Error verificando servicios: {e}")
        return False

def main():
    print("=" * 60)
    print("       MOVIES PRODUCER - YARN & HDFS")
    print("=" * 60)
    
    # Verificar servicios
    if not verificar_servicios():
        print("‚ùå Los servicios no est√°n disponibles. Ejecuta start_services.sh primero")
        return
    
    # Cargar datos desde JSON
    print("\n1. üì• CARGANDO DATOS DESDE JSON...")
    try:
        with open('movies.json', 'r', encoding='utf-8') as f:
            movies_data = json.load(f)
        
        movies_df = spark.createDataFrame(movies_data, schema=movies_schema)
        print(f"‚úÖ {movies_df.count()} pel√≠culas cargadas")
        movies_df.show(5)
        
    except Exception as e:
        print(f"‚ùå Error cargando datos: {e}")
        return
    
    # Escribir a HDFS usando YARN
    print("\n2. üì§ ESCRIBIENDO A HDFS CON YARN...")
    try:
        # Escribir en formato Parquet a HDFS
        movies_df.write \
            .mode("overwrite") \
            .format("parquet") \
            .save("hdfs://localhost:9000/user/movies/raw/movies_dataset")
        
        print("‚úÖ Datos escritos en HDFS (Parquet)")
        
        # Tambi√©n escribir en JSON
        movies_df.write \
            .mode("overwrite") \
            .format("json") \
            .save("hdfs://localhost:9000/user/movies/raw/movies_json")
        
        print("‚úÖ Datos escritos en HDFS (JSON)")
        
    except Exception as e:
        print(f"‚ùå Error escribiendo a HDFS: {e}")
        return
    
    # Verificar datos en HDFS
    print("\n3. üîç VERIFICANDO DATOS EN HDFS...")
    try:
        # Leer desde HDFS para verificaci√≥n
        hdfs_df = spark.read \
            .format("parquet") \
            .load("hdfs://localhost:9000/user/movies/raw/movies_dataset")
        
        print(f"‚úÖ Verificaci√≥n exitosa: {hdfs_df.count()} registros en HDFS")
        print("üìä Muestra de datos en HDFS:")
        hdfs_df.show(3)
        
    except Exception as e:
        print(f"‚ùå Error verificando HDFS: {e}")
        return
    
    print("\nüéâ PRODUCER COMPLETADO CON YARN Y HDFS!")
    print("üìç HDFS: hdfs://localhost:9000/user/movies/raw/")
    print("üìç YARN UI: http://localhost:8088")

if __name__ == "__main__":
    main()
    spark.stop()