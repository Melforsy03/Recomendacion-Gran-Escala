# Usa Spark oficial (versión estable)
FROM apache/spark:3.5.1

# ========== Versiones ==========
ARG HADOOP_VERSION=3.3.1
ENV JAVA_VERSION=11

# ========== Entorno Spark/Hadoop ==========
ENV SPARK_HOME=/opt/spark \
    HADOOP_HOME=/opt/hadoop \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    PYSPARK_PYTHON=python3 \
    PYSPARK_DRIVER_PYTHON=python3

ENV PATH="${SPARK_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}"

# ========== Sistema base ==========
USER root
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
      python3-pip \
      openjdk-11-jdk-headless \
      curl wget tar procps ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# JAVA_HOME (ruta típica en Debian/Ubuntu)
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# ========== Dependencias Python ==========
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# ========== Instalar Hadoop (desde carpeta local) ==========
WORKDIR /opt

# Copiar tu Hadoop descomprimido localmente (la carpeta debe estar junto al Dockerfile)
COPY hadoop-3.3.1 /opt/hadoop-3.3.1

# Crear enlace simbólico /opt/hadoop → /opt/hadoop-3.3.1
RUN ln -s /opt/hadoop-3.3.1 /opt/hadoop

# Crear directorio de configuración (por si no existe)
RUN mkdir -p "${HADOOP_CONF_DIR}"

# Evitar warnings en modo pseudo-distribuido
ENV HDFS_NAMENODE_USER=root \
    HDFS_DATANODE_USER=root \
    HDFS_SECONDARYNAMENODE_USER=root \
    YARN_RESOURCEMANAGER_USER=root \
    YARN_NODEMANAGER_USER=root

# ========== Configuración Hadoop ==========
COPY config/hadoop/core-site.xml     "${HADOOP_CONF_DIR}/core-site.xml"
COPY config/hadoop/hdfs-site.xml     "${HADOOP_CONF_DIR}/hdfs-site.xml"
COPY config/hadoop/yarn-site.xml     "${HADOOP_CONF_DIR}/yarn-site.xml"
COPY config/hadoop/mapred-site.xml   "${HADOOP_CONF_DIR}/mapred-site.xml"
COPY config/hadoop/workers           "${HADOOP_CONF_DIR}/workers"

# Directorios de datos HDFS
RUN mkdir -p /opt/hadoop_data/nn /opt/hadoop_data/dn

# Integración Spark <-> Hadoop
ENV SPARK_DIST_CLASSPATH="$(/opt/hadoop/bin/hadoop classpath)"

# ========== App y scripts ==========
WORKDIR /app
COPY . /app

RUN sh -c 'chmod +x /app/scripts/*.sh' || true

# Puertos web útiles
EXPOSE 9870 9864 8088 8042 4040

# Por defecto: arranca HDFS + YARN y mantiene contenedor vivo
CMD ["/app/scripts/start_hadoop_yarn.sh"]
