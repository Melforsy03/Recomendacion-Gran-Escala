# Usa Spark oficial (versión estable)
FROM apache/spark:3.5.1

# ========== Versiones ==========
ARG HADOOP_VERSION=3.3.1
ENV JAVA_VERSION=11

# ========== Entorno Spark/Hadoop ==========
ENV SPARK_HOME=/opt/spark \
    HADOOP_HOME=/opt/hadoop \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    PYSPARK_PYTHON=python3 \
    PYSPARK_DRIVER_PYTHON=python3

ENV PATH="${SPARK_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}"

# ========== Sistema base ==========
USER root
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
      python3-pip \
      openjdk-11-jdk-headless \
      curl wget tar procps ca-certificates \
      openssh-server openssh-client \
      sudo \
    && rm -rf /var/lib/apt/lists/*

# JAVA_HOME (ruta típica en Debian/Ubuntu)
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# ========== CONFIGURACIÓN SSH ==========
# Configurar SSH sin contraseña para Hadoop
RUN mkdir -p /var/run/sshd && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys

# Configurar SSH para localhost sin verificación
RUN echo "Host localhost" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile /dev/null" >> ~/.ssh/config

# Configurar acceso SSH sin contraseña para root
RUN echo 'root:root' | chpasswd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config

# ========== Dependencias Python ==========
COPY requirements.txt /tmp/requirements.txt
RUN pip install -r /tmp/requirements.txt
RUN pip install dash plotly kafka-python

# ========== Instalar Hadoop (desde carpeta local) ==========
WORKDIR /opt

# Copiar tu Hadoop descomprimido localmente
COPY hadoop-3.3.1 /opt/hadoop-3.3.1

# Crear enlace simbólico /opt/hadoop → /opt/hadoop-3.3.1
RUN ln -s /opt/hadoop-3.3.1 /opt/hadoop

# Crear directorio de configuración
RUN mkdir -p "${HADOOP_CONF_DIR}"

# Evitar warnings en modo pseudo-distribuido
ENV HDFS_NAMENODE_USER=root \
    HDFS_DATANODE_USER=root \
    HDFS_SECONDARYNAMENODE_USER=root \
    YARN_RESOURCEMANAGER_USER=root \
    YARN_NODEMANAGER_USER=root

# ========== Configuración Hadoop ==========
COPY config/hadoop/core-site.xml     "${HADOOP_CONF_DIR}/core-site.xml"
COPY config/hadoop/hdfs-site.xml     "${HADOOP_CONF_DIR}/hdfs-site.xml"
COPY config/hadoop/yarn-site.xml     "${HADOOP_CONF_DIR}/yarn-site.xml"
COPY config/hadoop/mapred-site.xml   "${HADOOP_CONF_DIR}/mapred-site.xml"
COPY config/hadoop/workers           "${HADOOP_CONF_DIR}/workers"
COPY films /app/films
# Directorios de datos HDFS
RUN mkdir -p /opt/hadoop_data/nn /opt/hadoop_data/dn

# Integración Spark <-> Hadoop
ENV SPARK_DIST_CLASSPATH="$(/opt/hadoop/bin/hadoop classpath)"

# ========== App y scripts ==========
WORKDIR /app
COPY . /app

RUN sh -c 'chmod +x /app/scripts/*.sh' || true

# Puertos web útiles 
EXPOSE 9870 9864 8088 8042 4040 9092 2181 22 8050

# --- Corregir JAVA_HOME en Hadoop ---
RUN sed -i 's|/usr/lib/jvm/java-8-openjdk-amd64|/usr/lib/jvm/java-11-openjdk-amd64|g' /opt/hadoop/etc/hadoop/hadoop-env.sh
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH
# --- Añadir conectores Kafka-Spark ---
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar -P /opt/spark/jars/ && \
    wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar -P /opt/spark/jars/ && \
    wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar -P /opt/spark/jars/ && \
    wget https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar -P /opt/spark/jars/ && \
    wget https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar -P /opt/spark/jars/

# Por defecto: arranca SSH + HDFS + YARN y mantiene contenedor vivo
CMD service ssh start && /app/scripts/start_all.sh