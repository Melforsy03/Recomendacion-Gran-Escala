# Usa Spark oficial (versi√≥n estable)
FROM apache/spark:3.5.1

# ========== Versiones ==========
ARG HADOOP_VERSION=3.3.1
ENV JAVA_VERSION=11

# ========== Entorno Spark/Hadoop ==========
ENV SPARK_HOME=/opt/spark \
    HADOOP_HOME=/opt/hadoop \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    PYSPARK_PYTHON=python3 \
    PYSPARK_DRIVER_PYTHON=python3

ENV PATH="${SPARK_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}"

# ========== Sistema base ==========
USER root
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
      python3-pip \
      openjdk-11-jdk-headless \
      curl wget tar procps ca-certificates \
      # üî• AGREGAR SSH
      openssh-server openssh-client \
      sudo \
    && rm -rf /var/lib/apt/lists/*

# JAVA_HOME (ruta t√≠pica en Debian/Ubuntu)
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# ========== CONFIGURACI√ìN SSH ==========
# Configurar SSH sin contrase√±a para Hadoop
RUN mkdir -p /var/run/sshd && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys

# Configurar SSH para localhost sin verificaci√≥n
RUN echo "Host localhost" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile /dev/null" >> ~/.ssh/config

# Configurar acceso SSH sin contrase√±a para root
RUN echo 'root:root' | chpasswd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config

# ========== Dependencias Python ==========
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# ========== Instalar Hadoop (desde carpeta local) ==========
WORKDIR /opt

# Copiar tu Hadoop descomprimido localmente
COPY hadoop-3.3.1 /opt/hadoop-3.3.1

# Crear enlace simb√≥lico /opt/hadoop ‚Üí /opt/hadoop-3.3.1
RUN ln -s /opt/hadoop-3.3.1 /opt/hadoop

# Crear directorio de configuraci√≥n
RUN mkdir -p "${HADOOP_CONF_DIR}"

# Evitar warnings en modo pseudo-distribuido
ENV HDFS_NAMENODE_USER=root \
    HDFS_DATANODE_USER=root \
    HDFS_SECONDARYNAMENODE_USER=root \
    YARN_RESOURCEMANAGER_USER=root \
    YARN_NODEMANAGER_USER=root

# ========== Configuraci√≥n Hadoop ==========
COPY config/hadoop/core-site.xml     "${HADOOP_CONF_DIR}/core-site.xml"
COPY config/hadoop/hdfs-site.xml     "${HADOOP_CONF_DIR}/hdfs-site.xml"
COPY config/hadoop/yarn-site.xml     "${HADOOP_CONF_DIR}/yarn-site.xml"
COPY config/hadoop/mapred-site.xml   "${HADOOP_CONF_DIR}/mapred-site.xml"
COPY config/hadoop/workers           "${HADOOP_CONF_DIR}/workers"

# Directorios de datos HDFS
RUN mkdir -p /opt/hadoop_data/nn /opt/hadoop_data/dn

# Integraci√≥n Spark <-> Hadoop
ENV SPARK_DIST_CLASSPATH="$(/opt/hadoop/bin/hadoop classpath)"

# ========== App y scripts ==========
WORKDIR /app
COPY . /app

RUN sh -c 'chmod +x /app/scripts/*.sh' || true

# Puertos web √∫tiles + SSH
EXPOSE 9870 9864 8088 8042 4040 9092 2181 22

# Por defecto: arranca SSH + HDFS + YARN y mantiene contenedor vivo
CMD service ssh start && /app/scripts/start_all.sh